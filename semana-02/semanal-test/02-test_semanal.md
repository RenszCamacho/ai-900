# ğŸ¯ SIMULACRO DE ESCENARIOS - AI-900 (Semana 2)

## Simulacro #1 - Scenario-Based Questions

**Tipo de preguntas:** Escenarios de negocio realistas  
**Cantidad:** 25 preguntas agrupadas en 8 escenarios  
**Tiempo sugerido:** 40 minutos  
**Estilo:** Microsoft AI-900 oficial

---

## ğŸ“‹ INSTRUCCIONES

**Formato de escenarios:**

- Cada escenario tiene 2-4 preguntas relacionadas
- Lee TODO el escenario antes de responder
- Las preguntas estÃ¡n conectadas (como en el examen real)
- Anota tus respuestas en tu cuaderno
- NO mires las respuestas hasta terminar

**AnotaciÃ³n sugerida:**

```
SIMULACRO ESCENARIOS #1
Fecha: ___________

1. __    6. __    11. __    16. __    21. __
2. __    7. __    12. __    17. __    22. __
3. __    8. __    13. __    18. __    23. __
4. __    9. __    14. __    19. __    24. __
5. __    10. __   15. __    20. __    25. __
```

---

# ğŸ“ ESCENARIOS Y PREGUNTAS

---

## ğŸ¥ ESCENARIO 1: Sistema de diagnÃ³stico mÃ©dico

**Contexto:**
You work for a hospital that wants to develop an AI system to help doctors diagnose pneumonia from chest X-ray images. The hospital has a dataset of 50,000 X-ray images:

- 45,000 images of healthy lungs (90%)
- 5,000 images showing pneumonia (10%)

Each image is labeled by expert radiologists as either "healthy" or "pneumonia".

Your initial model produces the following results:

```
Confusion Matrix:
                  Predicted
              Healthy  Pneumonia
Actual  H      8,900      100
        P        400      600

Metrics:
- Accuracy: 95%
- Precision: 85.7%
- Recall: 60%
- F1 Score: 70.6%
- AUC: 0.88
```

---

### Question 1

What type of machine learning is most appropriate for this pneumonia detection system?

- [ ] A) Unsupervised learning
- [x] B) Supervised learning
- [ ] C) Reinforcement learning
- [ ] D) Semi-supervised learning

---

### Question 2

The hospital is most concerned about missing pneumonia cases (patients who have pneumonia but are diagnosed as healthy). Which metric indicates the severity of this problem?

- [ ] A) Precision of 85.7%
- [x] B) Recall of 60%
- [ ] C) Accuracy of 95%
- [ ] D) AUC of 0.88

---

### Question 3

What does the Recall value of 60% mean in this medical context?

- [ ] A) 60% of patients predicted to have pneumonia actually have it
- [x] B) The model correctly identifies 60% of all actual pneumonia cases
- [ ] C) The model is 60% accurate overall
- [ ] D) 60% of healthy patients are correctly identified

---

### Question 4

The hospital director says: "We cannot afford to miss pneumonia cases. False negatives could be fatal." Which metric should you prioritize when optimizing the model?

- [ ] A) Accuracy
- [ ] B) Precision
- [x] C) Recall
- [ ] D) F1 Score

---

## ğŸ¦ ESCENARIO 2: Sistema bancario de aprobaciÃ³n de prÃ©stamos

**Contexto:**
A bank wants to build an AI model to automate loan approval decisions. They have historical data from 100,000 past loan applications with the following information for each application:

- Applicant age, income, employment history, credit score
- Whether the loan was approved or rejected
- Whether the borrower defaulted (if loan was approved)

The bank wants to predict whether a new applicant should be approved or rejected based on their likelihood to repay.

---

### Question 5

What type of machine learning task is this?

- [ ] A) Regression
- [x] B) Classification
- [ ] C) Clustering
- [ ] D) Anomaly detection

---

### Question 6

You train a model and achieve the following metrics:

- Accuracy: 92%
- Precision (for approval): 78%
- Recall (for approval): 85%

The bank is concerned about approving loans to people who will default. Which metric best addresses this concern?

- [ ] A) Accuracy - shows overall performance
- [x] B) Precision - shows reliability when approving loans
- [ ] C) Recall - shows how many good applicants we approve
- [ ] D) F1 Score - balances both concerns

---

### Question 7

The bank decides that missing good customers (false negatives) is less costly than approving bad loans (false positives). How should you adjust the model?

- [x] A) Increase the classification threshold to improve Precision
- [ ] B) Decrease the classification threshold to improve Recall
- [ ] C) Maximize Accuracy regardless of Precision/Recall
- [ ] D) Use clustering instead of classification

---

## ğŸ­ ESCENARIO 3: PredicciÃ³n de mantenimiento en fÃ¡brica

**Contexto:**
A manufacturing company wants to predict when industrial machines will need maintenance. They have collected 2 years of sensor data from 500 machines:

- Temperature readings every minute
- Vibration levels
- Hours of operation
- When each machine actually failed or required maintenance

The goal is to predict "hours until maintenance needed" for each machine.

---

### Question 8

What type of machine learning task is this?

- [ ] A) Classification
- [x] B) Regression
- [ ] C) Clustering
- [ ] D) Reinforcement learning

---

### Question 9

Your model produces the following metrics:

- RMSE: 48 hours
- MAE: 32 hours
- RÂ²: 0.73

Machines typically run 2,000-3,000 hours between maintenance. How should you interpret the RÂ² value?

- [ ] A) The model has 73% accuracy
- [x] B) The model explains 73% of the variance in maintenance timing
- [ ] C) 73% of predictions are within 48 hours
- [ ] D) The model will be correct 73% of the time

---

### Question 10

The factory manager asks: "What does RMSE of 48 hours mean practically?" What is the best explanation?

- [ ] A) The model will be wrong 48% of the time
- [x] B) On average, predictions are off by approximately 48 hours
- [ ] C) The model predicts exactly 48 hours for all machines
- [ ] D) 48% of machines need maintenance

---

## ğŸ›’ ESCENARIO 4: Sistema de recomendaciÃ³n de e-commerce

**Contexto:**
An e-commerce company wants to build a recommendation system. They have:

- Purchase history for 10 million customers
- Product catalog with 500,000 items
- Customer browsing behavior and search history

They want to group customers with similar purchasing patterns to provide better recommendations.

---

### Question 11

What type of machine learning approach is most suitable for finding customer groups?

- [ ] A) Supervised learning - Classification
- [ ] B) Supervised learning - Regression
- [x] C) Unsupervised learning - Clustering
- [ ] D) Reinforcement learning

---

### Question 12

After grouping customers, the company wants to predict which products a customer will buy next based on their group membership and past purchases. What type of machine learning is this?

- [ ] A) Unsupervised learning
- [x] B) Supervised learning
- [ ] C) Reinforcement learning
- [ ] D) Transfer learning

---

## ğŸš— ESCENARIO 5: Azure AutoML para predicciÃ³n de precios de autos

**Contexto:**
You are building a car price prediction model using Azure Automated ML. Your dataset contains:

- 50,000 used car listings
- Features: brand, model, year, mileage, condition, location
- Target: selling price

You configure the AutoML experiment with:

- Task type: Regression
- Primary metric: normalized_root_mean_squared_error
- Training job time: 2 hours
- Metric score threshold: 0.15
- Enable early stopping: Yes
- Max concurrent iterations: 4

---

### Question 13

When will the AutoML experiment stop training?

- [ ] A) Exactly after 2 hours
- [ ] B) After trying all possible algorithms
- [x] C) After 2 hours OR when normalized RMSE reaches 0.15, whichever comes first
- [ ] D) When it has trained exactly 4 models

---

### Question 14

The experiment completes with these results for the top 3 models:

```
1. VotingEnsemble:    RMSE = 0.12, RÂ² = 0.89
2. StackEnsemble:     RMSE = 0.13, RÂ² = 0.87
3. LightGBM:          RMSE = 0.14, RÂ² = 0.86
```

Which model will AutoML select as the best?

- [x] A) VotingEnsemble - lowest RMSE (0.12)
- [ ] B) VotingEnsemble - highest RÂ² (0.89)
- [ ] C) StackEnsemble - it's more sophisticated
- [ ] D) LightGBM - it's the fastest

---

### Question 15

What does "Enable early stopping: Yes" accomplish in this experiment?

- [ ] A) Stops immediately if any model fails
- [x] B) Stops training individual models if they stop improving
- [ ] C) Stops the entire experiment after the first model
- [ ] D) Prevents overfitting in all models

---

### Question 16

After the experiment completes, you want to understand which features most influence car prices. Where should you look in Azure ML Studio?

- [ ] A) The Confusion Matrix tab
- [x] B) The Explanations tab (Feature Importance)
- [ ] C) The ROC Curve tab
- [ ] D) The Compute tab

---

## ğŸ“§ ESCENARIO 6: Filtro de spam empresarial

**Contexto:**
A company receives 1 million emails per day. They want to build a spam filter with these requirements:

**Business requirements:**

- Critical work emails MUST NOT be blocked (highest priority)
- Some spam in inbox is acceptable
- Blocked emails go to quarantine (can be reviewed)

Your spam detection model produces:

```
Out of 10,000 test emails:
- 7,000 legitimate emails
- 3,000 spam emails

Confusion Matrix:
                  Predicted
              Legitimate  Spam
Actual  Leg      6,860    140
        Spam       450  2,550

Metrics:
- Accuracy: 94.1%
- Precision: 94.8%
- Recall: 85%
```

---

### Question 17

What does the Precision of 94.8% tell you about this spam filter?

- [ ] A) 94.8% of all emails are classified correctly
- [x] B) 94.8% of emails marked as spam are actually spam
- [ ] C) 94.8% of spam emails are correctly detected
- [ ] D) The filter blocks 94.8% of all emails

---

### Question 18

What is the main problem with this model given the business requirement that "critical work emails MUST NOT be blocked"?

- [ ] A) Recall is too low at 85%
- [ ] B) Accuracy is too low at 94.1%
- [x] C) There are 140 false positives (legitimate emails marked as spam)
- [ ] D) There are 450 false negatives (spam emails not detected)

---

### Question 19

How many legitimate work emails would be incorrectly blocked per day (false positives) if this model is deployed?

- [ ] A) 140 emails
- [ ] B) 450 emails
- [x] C) Approximately 14,000 emails
- [ ] D) Approximately 45,000 emails

---

### Question 20

Given the business requirement to prioritize not blocking legitimate emails, what should you do?

- [x] A) Increase the classification threshold to improve Precision further
- [ ] B) Decrease the classification threshold to improve Recall
- [ ] C) Focus on improving Accuracy to 99%
- [ ] D) Use a different primary metric like F1 Score

---

## ğŸ¥ ESCENARIO 7: Azure ML Workspace para investigaciÃ³n mÃ©dica

**Contexto:**
A medical research team wants to use Azure Machine Learning to analyze patient data and develop diagnostic models. They need to:

- Store large datasets from multiple hospitals (stored in Azure Blob Storage and Azure Data Lake)
- Run multiple training experiments in parallel
- Allow 5 data scientists to work simultaneously
- Track all experiment results and model versions
- Deploy the best models for clinical testing

They have an Azure subscription and need to set up their ML infrastructure.

---

### Question 21

What is the FIRST resource they should create in Azure?

- [ ] A) Compute Cluster
- [x] B) Azure ML Workspace
- [ ] C) Storage Account
- [ ] D) Virtual Machine

---

### Question 22

To connect their existing Azure Blob Storage (containing patient data) to Azure ML, what should they create?

- [ ] A) A new Storage Account
- [x] B) A Datastore
- [ ] C) A Compute Instance
- [ ] D) An Experiment

---

### Question 23

The team needs to run multiple model training jobs in parallel using different algorithms. Which compute resource should they provision?

- [ ] A) 5 separate Compute Instances (one per data scientist)
- [x] B) A Compute Cluster with min_nodes=0 and max_nodes=5
- [ ] C) A single powerful Virtual Machine
- [ ] D) An Inference Cluster

---

### Question 24

Where should the team look to compare metrics across all their experiments?

- [x] A) The Experiments section showing all runs
- [ ] B) The Models section in the registry
- [ ] C) The Datastores section
- [ ] D) The Compute section

---

### Question 25

After identifying the best model, they want to version it and prepare for deployment. Where should they register the model?

- [ ] A) In Azure Blob Storage
- [ ] B) In the Datastore
- [x] C) In the Model Registry
- [ ] D) In a new Experiment

---

## â¸ï¸ PAUSA - No continÃºes hasta terminar

**Â¿Terminaste las 25 preguntas?**

Si SÃ â†’ ContinÃºa para ver las respuestas  
Si NO â†’ No hagas scroll todavÃ­a

---

---

---

# ğŸ“Š RESPUESTAS Y EXPLICACIONES DETALLADAS

---

## ğŸ¥ ESCENARIO 1: Sistema de diagnÃ³stico mÃ©dico

### Respuesta 1: B) Supervised learning

**ExplicaciÃ³n:**
Tienes 50,000 imÃ¡genes **etiquetadas** por radiÃ³logos expertos (healthy o pneumonia). Esto es aprendizaje supervisado porque:

- Tienes datos de entrada (X-ray images)
- Tienes datos de salida conocidos (labels: healthy/pneumonia)
- El modelo aprende de estos ejemplos etiquetados

**TemÃ¡tica:** Tipos de ML

---

### Respuesta 2: B) Recall of 60%

**ExplicaciÃ³n:**
**Casos perdidos = Falsos Negativos (FN)**

```
FN = 400 (tienen pneumonia pero el modelo dice "healthy")

Recall = TP / (TP + FN) = 600 / (600 + 400) = 60%
```

**Recall del 60% significa:**

- El modelo solo detecta el 60% de los casos de pneumonia
- Se pierde el 40% (400 pacientes con pneumonia diagnosticados como sanos)

**Esto es GRAVE:** Pacientes enfermos se van a casa sin tratamiento.

**Por quÃ© otras no:**

- Precision: Mide confiabilidad cuando dice "pneumonia"
- Accuracy: Puede ser engaÃ±osa con clases desbalanceadas
- AUC: MÃ©trica general, no especÃ­fica para FN

**TemÃ¡tica:** ClasificaciÃ³n - MÃ©tricas

---

### Respuesta 3: B) The model correctly identifies 60% of all actual pneumonia cases

**ExplicaciÃ³n:**
**Recall = Sensibilidad = Â¿Detecto todos los casos positivos?**

```
Total casos reales de pneumonia: 1,000
Casos detectados correctamente: 600
Casos perdidos: 400

Recall = 600 / 1,000 = 60%
```

**En contexto mÃ©dico:**

- De cada 100 pacientes con pneumonia
- El modelo detecta 60
- Se le escapan 40

**TemÃ¡tica:** ClasificaciÃ³n - InterpretaciÃ³n de Recall

---

### Respuesta 4: C) Recall

**ExplicaciÃ³n:**
**"No podemos permitirnos perder casos de pneumonia"** â†’ Priorizar Recall

**Razonamiento:**

- **Falso Negativo (FN):** Paciente tiene pneumonia pero no se detecta â†’ FATAL (no recibe tratamiento)
- **Falso Positivo (FP):** Paciente sano diagnosticado con pneumonia â†’ ConfirmaciÃ³n con mÃ¡s pruebas

**En medicina:**

```
FN (no detectar enfermedad) >>> FP (falsa alarma)
```

**Estrategia:** Maximizar Recall aunque genere mÃ¡s falsos positivos (que se pueden verificar).

**TemÃ¡tica:** ClasificaciÃ³n - DecisiÃ³n de negocio

---

## ğŸ¦ ESCENARIO 2: Sistema bancario de aprobaciÃ³n de prÃ©stamos

### Respuesta 5: B) Classification

**ExplicaciÃ³n:**
Predices una **categorÃ­a binaria:**

- Aprobar (approve)
- Rechazar (reject)

**No es regresiÃ³n porque:**

- No predices un valor numÃ©rico (ej: "probabilidad de default")
- Predices una decisiÃ³n categÃ³rica

**Nota:** Internamente el modelo puede calcular probabilidades, pero la salida final es una clasificaciÃ³n.

**TemÃ¡tica:** Tipos de tareas ML

---

### Respuesta 6: B) Precision - shows reliability when approving loans

**ExplicaciÃ³n:**
**"Preocupados por aprobar prÃ©stamos a gente que harÃ¡ default"** â†’ Falsos Positivos

**Precision responde:**
"De todos los prÃ©stamos que APROBAMOS, Â¿cuÃ¡ntos realmente serÃ¡n pagados?"

```
Precision = 78%

Significa:
- De cada 100 prÃ©stamos aprobados
- 78 serÃ¡n pagados (TP)
- 22 harÃ¡n default (FP) â† Problema del banco
```

**Precision baja = Muchos prÃ©stamos malos aprobados = PÃ©rdidas financieras**

**Por quÃ© otras no:**

- Accuracy: MÃ©trica general, no especÃ­fica para este riesgo
- Recall: Mide cuÃ¡ntos buenos clientes detectamos (menos crÃ­tico)
- F1: Balance, pero aquÃ­ hay prioridad clara

**TemÃ¡tica:** ClasificaciÃ³n - AplicaciÃ³n de negocio

---

### Respuesta 7: A) Increase the classification threshold to improve Precision

**ExplicaciÃ³n:**
**"Perder buenos clientes es menos costoso que aprobar malos prÃ©stamos"** â†’ Priorizar Precision

**Ajuste de umbral:**

**Umbral normal (0.5):**

```
Probabilidad > 0.5 â†’ Aprobar
```

**Umbral alto (0.7):**

```
Probabilidad > 0.7 â†’ Aprobar (mÃ¡s conservador)

Efecto:
âœ… Menos FP (menos prÃ©stamos malos aprobados)
âœ… Mayor Precision
âŒ MÃ¡s FN (rechazamos algunos buenos clientes)
âŒ Menor Recall
```

**DecisiÃ³n de negocio:** Ser conservador, aprobar solo cuando estÃ©s muy seguro.

**TemÃ¡tica:** ClasificaciÃ³n - OptimizaciÃ³n de modelo

---

## ğŸ­ ESCENARIO 3: PredicciÃ³n de mantenimiento en fÃ¡brica

### Respuesta 8: B) Regression

**ExplicaciÃ³n:**
Predices **"horas hasta mantenimiento"** = Valor numÃ©rico continuo

**CaracterÃ­sticas de regresiÃ³n:**

- Variable objetivo: numÃ©rica (48 horas, 1,234 horas, etc.)
- Puede tomar cualquier valor en un rango
- No son categorÃ­as discretas

**Si fuera clasificaciÃ³n:** PredirÃ­as "necesita mantenimiento pronto/tarde/nunca"

**TemÃ¡tica:** Tipos de tareas ML

---

### Respuesta 9: B) The model explains 73% of the variance in maintenance timing

**ExplicaciÃ³n:**
**RÂ² = 0.73 (Coeficiente de determinaciÃ³n)**

**Significa:**

- El modelo explica el 73% de la variabilidad en cuÃ¡ndo las mÃ¡quinas necesitan mantenimiento
- El 27% restante no es explicado (factores externos, aleatoriedad, etc.)

**InterpretaciÃ³n:**

- RÂ² = 0.73 es un modelo **bueno/moderado**
- Para mantenimiento predictivo es aceptable
- PodrÃ­a mejorarse pero es Ãºtil

**NO significa:**

- âŒ 73% de accuracy
- âŒ 73% de predicciones correctas
- âŒ Error del 27%

**TemÃ¡tica:** RegresiÃ³n - InterpretaciÃ³n de RÂ²

---

### Respuesta 10: B) On average, predictions are off by approximately 48 hours

**ExplicaciÃ³n:**
**RMSE = 48 horas**

**En tÃ©rminos prÃ¡cticos:**

- Si predices que una mÃ¡quina necesita mantenimiento en 1,000 horas
- El valor real probablemente estarÃ¡ entre 952-1,048 horas
- Error promedio tÃ­pico: Â±48 horas

**Contexto:**

- MÃ¡quinas operan 2,000-3,000 horas entre mantenimientos
- Error de 48 horas = ~2-3% del ciclo
- **Es razonable** para planificaciÃ³n de mantenimiento

**Por quÃ© otras no:**

- âŒ "Wrong 48% of the time": RMSE no es un porcentaje de error
- âŒ "Predicts exactly 48 hours": RMSE es una medida de error, no una predicciÃ³n
- âŒ "48% need maintenance": No tiene relaciÃ³n

**TemÃ¡tica:** RegresiÃ³n - InterpretaciÃ³n prÃ¡ctica de RMSE

---

## ğŸ›’ ESCENARIO 4: Sistema de recomendaciÃ³n de e-commerce

### Respuesta 11: C) Unsupervised learning - Clustering

**ExplicaciÃ³n:**
**"Agrupar clientes con patrones similares"** = Clustering (unsupervised)

**Por quÃ© unsupervised:**

- NO tienes etiquetas predefinidas de "grupos de clientes"
- Quieres que el algoritmo **descubra** automÃ¡ticamente los grupos
- No sabes de antemano cuÃ¡ntos grupos hay o cÃ³mo son

**Algoritmos tÃ­picos:**

- K-means
- Hierarchical clustering
- DBSCAN

**Resultado:** Clientes agrupados por similitud en comportamiento de compra.

**TemÃ¡tica:** Tipos de ML - Unsupervised learning

---

### Respuesta 12: B) Supervised learning

**ExplicaciÃ³n:**
**"Predecir quÃ© productos comprarÃ¡"** basado en datos histÃ³ricos = Supervised learning

**Por quÃ© supervised:**

- Tienes datos histÃ³ricos: quÃ© productos comprÃ³ cada cliente (labels conocidos)
- El modelo aprende de estos ejemplos
- Predice compras futuras basÃ¡ndose en patrones aprendidos

**Flujo completo:**

1. **Unsupervised:** Agrupar clientes similares (clustering)
2. **Supervised:** Predecir compras futuras basÃ¡ndose en grupo y historial

**TemÃ¡tica:** Tipos de ML - CombinaciÃ³n de enfoques

---

## ğŸš— ESCENARIO 5: Azure AutoML para predicciÃ³n de precios de autos

### Respuesta 13: C) After 2 hours OR when normalized RMSE reaches 0.15, whichever comes first

**ExplicaciÃ³n:**
**Exit Criterion** define cuÃ¡ndo parar:

**Dos condiciones:**

1. **Training job time:** 2 hours
2. **Metric score threshold:** 0.15 (normalized RMSE)

**El experimento para cuando se cumple CUALQUIERA:**

**Escenario A:**

```
Hora 1: Mejor RMSE = 0.20
Hora 1.5: Mejor RMSE = 0.14 â† AlcanzÃ³ 0.15
â†’ PARA (solo tardÃ³ 1.5 horas)
```

**Escenario B:**

```
Hora 1: Mejor RMSE = 0.25
Hora 2: Mejor RMSE = 0.18 â† Tiempo agotado
â†’ PARA (no alcanzÃ³ 0.15 pero se acabÃ³ el tiempo)
```

**TemÃ¡tica:** AutoML - Exit Criterion

---

### Respuesta 14: A) VotingEnsemble - lowest RMSE (0.12)

**ExplicaciÃ³n:**
**Primary metric configurada:** normalized_root_mean_squared_error

**AutoML selecciona el modelo con el MEJOR valor de la primary metric:**

```
Primary metric: RMSE (lower is better)

VotingEnsemble:  RMSE = 0.12 â† MEJOR (mÃ¡s bajo)
StackEnsemble:   RMSE = 0.13
LightGBM:        RMSE = 0.14
```

**Ganador:** VotingEnsemble (RMSE mÃ¡s bajo)

**Nota:** Aunque VotingEnsemble tambiÃ©n tiene el RÂ² mÃ¡s alto (0.89), la decisiÃ³n se basa en la primary metric (RMSE), no en RÂ².

**TemÃ¡tica:** AutoML - Primary Metric

---

### Respuesta 15: B) Stops training individual models if they stop improving

**ExplicaciÃ³n:**
**Early Stopping:**

- Monitorea cada modelo durante su entrenamiento
- Si un modelo deja de mejorar despuÃ©s de X iteraciones/epochs
- Para ese modelo especÃ­ficamente (no todo el experimento)

**Beneficios:**

- â±ï¸ Ahorra tiempo de compute
- ğŸ’° Reduce costos
- ğŸ¯ Evita overfitting

**Ejemplo:**

```
LightGBM training:
- Epoch 1-10: RMSE mejora (0.30 â†’ 0.18)
- Epoch 11-20: RMSE estancado (0.18 â†’ 0.18)
â†’ EARLY STOP: Para LightGBM en epoch 20
â†’ AutoML continÃºa con otros algoritmos
```

**NO para todo el experimento, solo modelos individuales que no mejoran.**

**TemÃ¡tica:** AutoML - Early Stopping

---

### Respuesta 16: B) The Explanations tab (Feature Importance)

**ExplicaciÃ³n:**
**Feature Importance** muestra quÃ© caracterÃ­sticas mÃ¡s influyen en las predicciones.

**En Azure ML Studio:**

```
Best Model â†’ Tab "Explanations" â†’ Feature Importance

Ejemplo de resultado:
1. Mileage:     0.32 (32%) â† MÃS IMPORTANTE
2. Brand:       0.25 (25%)
3. Year:        0.18 (18%)
4. Condition:   0.15 (15%)
5. Model:       0.10 (10%)
```

**InterpretaciÃ³n:**

- El kilometraje (mileage) es el factor mÃ¡s importante en el precio
- La marca tambiÃ©n influye significativamente
- El aÃ±o del auto tiene impacto moderado

**Por quÃ© otras no:**

- Confusion Matrix: Solo para clasificaciÃ³n
- ROC Curve: Solo para clasificaciÃ³n
- Compute: GestiÃ³n de recursos, no insights del modelo

**TemÃ¡tica:** AutoML - Interpretabilidad

---

## ğŸ“§ ESCENARIO 6: Filtro de spam empresarial

### Respuesta 17: B) 94.8% of emails marked as spam are actually spam

**ExplicaciÃ³n:**
**Precision responde:** "De todo lo que marquÃ© como spam, Â¿cuÃ¡nto realmente es spam?"

```
Precision = TP / (TP + FP)

TP = 2,550 (spam detectado correctamente)
FP = 140 (legÃ­timo marcado como spam)

Precision = 2,550 / (2,550 + 140) = 2,550 / 2,690 = 94.8%
```

**Significa:**

- De cada 1,000 emails que el filtro marca como spam
- 948 realmente son spam âœ…
- 52 son legÃ­timos âŒ (bloqueados incorrectamente)

**TemÃ¡tica:** ClasificaciÃ³n - InterpretaciÃ³n de Precision

---

### Respuesta 18: C) There are 140 false positives (legitimate emails marked as spam)

**ExplicaciÃ³n:**
**Requisito de negocio:** "Critical work emails MUST NOT be blocked"

**Problema del modelo:**

```
False Positives (FP) = 140 emails legÃ­timos bloqueados

En 10,000 emails:
- 140 emails de trabajo importantes van a cuarentena
- Los usuarios NO ven estos emails
- Pueden perder informaciÃ³n crÃ­tica
```

**Por quÃ© otras no:**

- **A) Recall 85%:** Se relaciona con spam no detectado (FN), no con emails legÃ­timos bloqueados
- **B) Accuracy 94.1%:** MÃ©trica general, no especÃ­fica al problema
- **D) 450 FN:** Spam que pasa al inbox es molesto, pero menos grave que bloquear trabajo importante

**TemÃ¡tica:** ClasificaciÃ³n - IdentificaciÃ³n de problemas de negocio

---

### Respuesta 19: C) Approximately 14,000 emails

**ExplicaciÃ³n:**
**CÃ¡lculo de extrapolaciÃ³n:**

```
Test set: 10,000 emails
- Legitimate: 7,000 (70%)
- False Positives: 140

Ratio FP: 140 / 7,000 = 0.02 = 2%

Daily volume: 1,000,000 emails
- Legitimate emails: 70% = 700,000
- Expected FP: 700,000 Ã— 0.02 = 14,000 emails
```

**Resultado:** ~14,000 emails legÃ­timos bloqueados por dÃ­a

**Impacto:**

- 14,000 emails importantes en cuarentena diariamente
- Requiere revisiÃ³n manual
- PÃ©rdida de productividad masiva

**TemÃ¡tica:** ClasificaciÃ³n - Impacto de negocio

---

### Respuesta 20: A) Increase the classification threshold to improve Precision further

**ExplicaciÃ³n:**
**Objetivo:** Minimizar FP (emails legÃ­timos bloqueados)

**Estrategia:** Aumentar umbral de clasificaciÃ³n

**Ajuste de umbral:**

**Actual (umbral ~0.5):**

```
Score > 0.5 â†’ Spam
Precision = 94.8%
FP = 140
```

**Nuevo (umbral 0.8):**

```
Score > 0.8 â†’ Spam (mÃ¡s conservador)

Efecto:
âœ… Mayor Precision (99%+)
âœ… Menos FP (~20-30 emails bloqueados incorrectamente)
âŒ Menor Recall (~70%)
âŒ MÃ¡s spam en inbox (aceptable segÃºn requisitos)
```

**DecisiÃ³n de negocio:**

- Prioridad #1: NO bloquear trabajo importante
- Aceptable: Algo de spam en inbox
- Trade-off correcto para el negocio

**Por quÃ© otras no:**

- **B) Decrease threshold:** AumentarÃ­a FP (peor)
- **C) Focus on Accuracy:** No resuelve el problema de FP
- **D) Use F1:** No cambia la priorizaciÃ³n del problema

**TemÃ¡tica:** ClasificaciÃ³n - OptimizaciÃ³n basada en negocio

---

## ğŸ¥ ESCENARIO 7: Azure ML Workspace para investigaciÃ³n mÃ©dica

### Respuesta 21: B) Azure ML Workspace

**ExplicaciÃ³n:**
**Azure ML Workspace** es el recurso fundamental y PRIMERO que se debe crear.

**Razones:**

1. **Es el contenedor central** para todos los demÃ¡s recursos
2. **Requiere:**
   - SuscripciÃ³n de Azure
   - Resource Group
   - RegiÃ³n
3. **Crea automÃ¡ticamente:** Storage Account, Key Vault, Application Insights

**Flujo correcto:**

```
1. Crear Workspace â† PRIMERO
2. Crear Datastores (conectar blob storage existente)
3. Crear Compute resources
4. Crear Experiments
```

**NO puedes crear Compute, Datastores o Experiments sin un Workspace.**

**TemÃ¡tica:** Azure ML - ConfiguraciÃ³n inicial

---

### Respuesta 22: B) A Datastore

**ExplicaciÃ³n:**
**Datastore** = ConexiÃ³n/referencia a servicios de almacenamiento existentes

**En este caso:**

```
Ya existe: Azure Blob Storage con datos de pacientes
Necesitas: Conectarlo a Azure ML

SoluciÃ³n: Crear un Datastore que:
- Referencias el Blob Storage existente
- Almacena credenciales de forma segura
- Permite acceso desde experimentos
```

**CÃ³digo ejemplo:**

```python
from azure.ai.ml.entities import AzureBlobDatastore

blob_datastore = AzureBlobDatastore(
    name="patient_data_store",
    account_name="hospitalstorage",
    container_name="patient-records",
    credentials=account_key
)

ml_client.datastores.create_or_update(blob_datastore)
```

**Por quÃ© otras no:**

- **A) New Storage Account:** Ya tienen storage, no necesitan otro
- **C) Compute Instance:** Para desarrollo, no para datos
- **D) Experiment:** Para tracking de runs, no para datos

**TemÃ¡tica:** Azure ML - Datastores

---

### Respuesta 23: B) A Compute Cluster with min_nodes=0 and max_nodes=5

**ExplicaciÃ³n:**
**Requisito:** "Run multiple training jobs in parallel"

**Compute Cluster caracterÃ­sticas:**

```
ConfiguraciÃ³n ideal:
- min_nodes: 0 (escala a cero cuando inactivo = ahorro)
- max_nodes: 5 (permite hasta 5 jobs en paralelo)
- VM size: Standard_DS3_v2 o similar

Beneficios:
âœ… Auto-scaling (0 a 5 nodos segÃºn demanda)
âœ… EjecuciÃ³n paralela de mÃºltiples experimentos
âœ… Cost-effective (pagas solo lo que usas)
âœ… Compartido entre todos los data scientists
```

**Por quÃ© otras no:**

- **A) 5 Compute Instances separados:**
  - MÃ¡s costoso (siempre activos)
  - No escalan automÃ¡ticamente
  - DifÃ­cil de administrar
- **C) Single VM:**
  - No permite paralelizaciÃ³n
  - No escala
- **D) Inference Cluster:**
  - Para deployment, no para training

**TemÃ¡tica:** Azure ML - Compute resources

---

### Respuesta 24: A) The Experiments section showing all runs

**ExplicaciÃ³n:**
**Experiments** agrupa y organiza mÃºltiples runs (ejecuciones).

**En Azure ML Studio:**

```
Experiments â†’ "Medical Diagnosis Research"

VerÃ¡s tabla con todos los runs:
Run ID    Algorithm         Accuracy  Precision  Recall  AUC
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
run_001   LogisticReg      0.82      0.78       0.85   0.88
run_002   RandomForest     0.87      0.84       0.88   0.92
run_003   XGBoost          0.89      0.86       0.90   0.94
run_004   VotingEnsemble   0.91      0.88       0.92   0.95

âœ… Comparar mÃ©tricas fÃ¡cilmente
âœ… Identificar el mejor modelo
âœ… Ver tendencias y progreso
```

**Funcionalidad:**

- Ordenar por cualquier mÃ©trica
- Filtrar runs
- Visualizar grÃ¡ficos comparativos
- Exportar resultados

**Por quÃ© otras no:**

- **B) Models:** Muestra modelos registrados, no todos los runs
- **C) Datastores:** Para gestiÃ³n de datos
- **D) Compute:** Para gestiÃ³n de recursos

**TemÃ¡tica:** Azure ML - Experiments y tracking

---

### Respuesta 25: C) In the Model Registry

**ExplicaciÃ³n:**
**Model Registry** = Repositorio centralizado para modelos entrenados

**PropÃ³sito:**

```
1. Registrar modelo:
   - Nombre: "pneumonia-detector"
   - VersiÃ³n: v1
   - MÃ©tricas: Accuracy=0.91, AUC=0.95
   - Archivos: model.pkl, preprocessing.pkl
   - Tags: "production-ready", "clinical-trial"

2. Versionado automÃ¡tico:
   - v1 â†’ Initial model
   - v2 â†’ Improved with more data
   - v3 â†’ Fine-tuned version

3. Metadata:
   - QuiÃ©n lo creÃ³
   - CuÃ¡ndo se creÃ³
   - De quÃ© experiment/run viene
   - HiperparÃ¡metros usados

4. Deployment:
   - Deploy directamente desde registry
   - Rollback a versiones anteriores
   - A/B testing entre versiones
```

**Beneficios:**

- âœ… Trazabilidad completa
- âœ… Versionado automÃ¡tico
- âœ… Facilita deployment
- âœ… AuditorÃ­a y compliance
- âœ… ColaboraciÃ³n entre equipo

**Por quÃ© otras no:**

- **A) Blob Storage:** Almacenamiento raw, sin versionado ni metadata
- **B) Datastore:** Para datos, no modelos
- **D) Experiment:** Para tracking de runs, no para modelos finales

**TemÃ¡tica:** Azure ML - Model Registry

---

# ğŸ“Š CALIFICACIÃ“N Y ANÃLISIS

## Tu puntuaciÃ³n:

```
Respuestas correctas: ___ / 25

Porcentaje: (correctas / 25) Ã— 100 = ____%
```

---

## ğŸ¯ InterpretaciÃ³n:

**23-25 correctas (92-100%):** ğŸŒŸ Excelente

- Dominas escenarios de negocio
- Aplicas conceptos correctamente
- Listo para el examen

**20-22 correctas (80-88%):** âœ… Muy Bueno

- Buena comprensiÃ³n de escenarios
- Revisa las que fallaste
- Casi listo

**17-19 correctas (68-76%):** ğŸ‘ Bueno

- ComprensiÃ³n sÃ³lida
- Necesitas practicar interpretaciÃ³n de negocio
- Repasa Ã¡reas dÃ©biles

**14-16 correctas (56-64%):** âš ï¸ Regular

- Entiendes conceptos tÃ©cnicos
- Dificultad aplicando a negocio
- Repasa escenarios

**<14 correctas (<56%):** ğŸ”„ Necesitas repasar

- Repasa material de Semana 2
- Practica mÃ¡s con escenarios
- EnfÃ³cate en aplicaciÃ³n prÃ¡ctica

---

## ğŸ“ˆ AnÃ¡lisis por escenario:

**Escenario 1 (Medicina - Q1-4):** **\_/4
**Escenario 2 (Banca - Q5-7):** \_**/3
**Escenario 3 (Manufactura - Q8-10):** **\_/3
**Escenario 4 (E-commerce - Q11-12):** \_**/2
**Escenario 5 (AutoML - Q13-16):** **\_/4
**Escenario 6 (Spam - Q17-20):** \_**/4
**Escenario 7 (Azure ML - Q21-25):** \_\_\_/5

---

## ğŸ’¡ Ãreas a reforzar segÃºn errores:

**Si fallaste en Escenarios 1 o 6:**
â†’ Repasa: Precision vs Recall, decisiones de negocio, trade-offs

**Si fallaste en Escenario 2:**
â†’ Repasa: Ajuste de umbrales, optimizaciÃ³n basada en costos

**Si fallaste en Escenario 3:**
â†’ Repasa: InterpretaciÃ³n de RMSE y RÂ² en contexto real

**Si fallaste en Escenario 4:**
â†’ Repasa: Diferencia entre supervised y unsupervised learning

**Si fallaste en Escenario 5:**
â†’ Repasa: AutoML (exit criterion, primary metric, feature importance)

**Si fallaste en Escenario 7:**
â†’ Repasa: Azure ML Workspace, Datastores, Compute, Model Registry

---

## ğŸŠ Â¡EXCELENTE TRABAJO!

**Has completado el Simulacro de Escenarios #1**

**Â¿Quieres el Simulacro #2 con mÃ¡s escenarios?** ğŸš€

---

_Simulacro creado: Domingo, Noviembre 2025_  
_Basado en: Semana 2 completa - Escenarios realistas_  
_Estilo: Microsoft AI-900 Scenario-Based Questions_
