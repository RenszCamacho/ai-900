# ğŸ“š SEMANA 2 - MACHINE LEARNING EN PROFUNDIDAD

---

## ğŸ“– MIÃ‰RCOLES 12 NOV (1.5 horas) - ClasificaciÃ³n y sus mÃ©tricas

### ğŸ¯ Objetivo del dÃ­a

Entender clasificaciÃ³n en profundidad y cÃ³mo evaluar si un modelo de clasificaciÃ³n es bueno

---

## ğŸ”„ REPASO RÃPIDO: RegresiÃ³n vs ClasificaciÃ³n

**AYER (Martes) - REGRESIÃ“N:**

- Predice: NÃšMEROS continuos
- Ejemplos: precio (150,000â‚¬), temperatura (25Â°C)
- MÃ©tricas: MAE, RMSE, RÂ²

**HOY (MiÃ©rcoles) - CLASIFICACIÃ“N:**

- Predice: CATEGORÃAS discretas
- Ejemplos: spam/no spam, gato/perro, sÃ­/no
- MÃ©tricas: Accuracy, Precision, Recall, F1-Score

---

## ğŸ·ï¸ Â¿QUÃ‰ ES CLASIFICACIÃ“N?

### ğŸ’¡ Concepto fundamental:

**ClasificaciÃ³n = asignar una etiqueta/categorÃ­a a cada dato**

**Pregunta que responde:** "Â¿A quÃ© GRUPO pertenece esto?"

---

### ğŸ“Š TIPOS DE CLASIFICACIÃ“N:

#### 1ï¸âƒ£ CLASIFICACIÃ“N BINARIA (2 categorÃ­as)

**Las mÃ¡s comunes:**

- âœ… / âŒ â†’ SÃ­ / No
- ğŸ“§ / ğŸ—‘ï¸ â†’ LegÃ­timo / Spam
- ğŸ˜Š / ğŸ˜¢ â†’ Positivo / Negativo
- âœ“ / âœ— â†’ Aprueba / No aprueba
- ğŸ’³ / ğŸš« â†’ Pago legÃ­timo / Fraude

**Ejemplos:**

1. Email spam detector
2. Detector de fraude en transacciones
3. DiagnÃ³stico mÃ©dico: enfermo / sano
4. Aprobar o rechazar prÃ©stamo
5. Cliente comprarÃ¡ / no comprarÃ¡

---

#### 2ï¸âƒ£ CLASIFICACIÃ“N MULTICLASE (3+ categorÃ­as)

**Ejemplos:**

- ğŸ• ğŸ± ğŸ¦ â†’ Perro / Gato / PÃ¡jaro
- ğŸ”´ ğŸŸ¢ ğŸ”µ â†’ Roja / Verde / Azul
- â­â­â­â­â­ â†’ 1 estrella / 2 / 3 / 4 / 5
- ğŸŒ â†’ Europa / Asia / AmÃ©rica / Ãfrica / OceanÃ­a

**Ejemplos reales:**

1. Clasificar tipos de flores (iris dataset)
2. Reconocer dÃ­gitos escritos a mano (0-9)
3. Clasificar noticias por categorÃ­a
4. Identificar idioma de un texto
5. DiagnÃ³stico mÃ©dico con mÃºltiples enfermedades

---

### ğŸ¯ Â¿CÃ³mo funciona un clasificador?

**Proceso:**

```
INPUT (datos)
    â†“
[MODELO CLASIFICADOR]
    â†“
OUTPUT (categorÃ­a predicha)
```

**Ejemplo concreto - Detector de spam:**

```
INPUT: Email
- Contiene: "Â¡Gana dinero rÃ¡pido!"
- Remitente: desconocido@spam.com
- 10 signos de exclamaciÃ³n
- 5 enlaces sospechosos
    â†“
[MODELO]
    â†“
OUTPUT: SPAM âœ… (Confianza: 98%)
```

---

## ğŸ­ LA MATRIZ DE CONFUSIÃ“N

### ğŸ“Š Â¿QuÃ© es?

**La herramienta CLAVE para evaluar clasificadores.**

Una tabla que muestra:

- Â¿QuÃ© predijo el modelo?
- Â¿QuÃ© era realmente?

---

### ğŸ’¡ EXPLICACIÃ“N CON EJEMPLO: Detector de Spam

**Imagina que evaluaste 100 emails:**

```
MATRIZ DE CONFUSIÃ“N

                    LO QUE REALMENTE ERA
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  LegÃ­timo   â”‚    Spam     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  LegÃ­timo   â”‚     50      â”‚      5      â”‚
LO  â”‚             â”‚   (TN)      â”‚    (FN)     â”‚
QUE â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
PRE â”‚    Spam     â”‚     10      â”‚     35      â”‚
DIJEâ”‚             â”‚   (FP)      â”‚    (TP)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Desglose:**

- **50 LegÃ­timos correctos (TN):** Predije "legÃ­timo" y ERA legÃ­timo âœ…
- **35 Spam correctos (TP):** Predije "spam" y ERA spam âœ…
- **10 Falsos positivos (FP):** Predije "spam" pero era legÃ­timo âŒ
- **5 Falsos negativos (FN):** Predije "legÃ­timo" pero era spam âŒ

---

### ğŸ”‘ LOS 4 TÃ‰RMINOS CLAVE:

#### 1ï¸âƒ£ TRUE POSITIVE (TP) - Verdadero Positivo âœ…âœ…

**"Dije que SÃ, y tenÃ­a razÃ³n"**

**Ejemplo spam:**

- Predije: "Esto ES spam"
- Realidad: "SÃ­, ERA spam"
- âœ… Â¡Acierto!

**Otros ejemplos:**

- DiagnÃ³stico: "Tienes la enfermedad" â†’ y sÃ­ la tiene
- Fraude: "Es fraude" â†’ y sÃ­ es fraude
- Calidad: "Producto defectuoso" â†’ y sÃ­ estÃ¡ defectuoso

---

#### 2ï¸âƒ£ TRUE NEGATIVE (TN) - Verdadero Negativo âœ…âœ…

**"Dije que NO, y tenÃ­a razÃ³n"**

**Ejemplo spam:**

- Predije: "Esto NO es spam"
- Realidad: "Correcto, NO era spam"
- âœ… Â¡Acierto!

**Otros ejemplos:**

- DiagnÃ³stico: "NO tienes la enfermedad" â†’ y no la tiene
- Fraude: "NO es fraude" â†’ y no es fraude
- Calidad: "Producto bueno" â†’ y sÃ­ estÃ¡ bueno

---

#### 3ï¸âƒ£ FALSE POSITIVE (FP) - Falso Positivo âŒ (Error Tipo I)

**"Dije que SÃ, pero me equivoquÃ©"**

**Ejemplo spam:**

- Predije: "Esto ES spam"
- Realidad: "No, era un email importante"
- âŒ Â¡Error grave! Perdiste un email importante

**Otros ejemplos:**

- DiagnÃ³stico: "Tienes cÃ¡ncer" â†’ pero NO lo tiene (alarma innecesaria)
- Fraude: "Es fraude" â†’ pero era una compra legÃ­tima (bloqueas al cliente)
- Alarma incendios: Suena â†’ pero NO hay fuego

**Consecuencia:** Falsa alarma, pÃ¡nico innecesario, molestias

---

#### 4ï¸âƒ£ FALSE NEGATIVE (FN) - Falso Negativo âŒ (Error Tipo II)

**"Dije que NO, pero me equivoquÃ©"**

**Ejemplo spam:**

- Predije: "Esto NO es spam"
- Realidad: "SÃ­ era spam" â†’ llega a tu bandeja
- âŒ Error molesto, pero menos grave

**Otros ejemplos:**

- DiagnÃ³stico: "NO tienes cÃ¡ncer" â†’ pero SÃ lo tiene (peligrosÃ­simo)
- Fraude: "NO es fraude" â†’ pero SÃ es fraude (pierdes dinero)
- Seguridad aeropuerto: "NO hay amenaza" â†’ pero SÃ la hay (desastre)

**Consecuencia:** Peligro no detectado, problema pasa desapercibido

---

### ğŸ¯ RESUMEN VISUAL:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  REALIDAD                           â”‚
â”‚              Positivo      Negativo                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PREDICCIÃ“N                                          â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  Positivo    â”‚    TP    â”‚    FP    â”‚                â”‚
â”‚              â”‚    âœ…âœ…  â”‚    âŒ    â”‚                â”‚
â”‚              â”‚ Â¡Acierto!â”‚ Falsa    â”‚                â”‚
â”‚              â”‚          â”‚ alarma   â”‚                â”‚
â”‚              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚
â”‚  Negativo    â”‚    FN    â”‚    TN    â”‚                â”‚
â”‚              â”‚    âŒ    â”‚    âœ…âœ…  â”‚                â”‚
â”‚              â”‚ Â¡PerdÃ­   â”‚ Â¡Acierto!â”‚                â”‚
â”‚              â”‚  algo!   â”‚          â”‚                â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ MÃ‰TRICAS DE CLASIFICACIÃ“N

### ğŸ¯ Las 4 mÃ©tricas principales:

```
MÃ‰TRICAS DE CLASIFICACIÃ“N
â”‚
â”œâ”€â”€ 1. Accuracy (PrecisiÃ³n Global)
â”œâ”€â”€ 2. Precision (PrecisiÃ³n)
â”œâ”€â”€ 3. Recall (Exhaustividad/Sensibilidad)
â””â”€â”€ 4. F1-Score (Balance entre Precision y Recall)
```

---

## 1ï¸âƒ£ ACCURACY (PrecisiÃ³n Global)

### ğŸ“Š Â¿QuÃ© mide?

**"Â¿QuÃ© porcentaje de predicciones fueron correctas?"**

**FÃ³rmula en palabras:**

```
Accuracy = (Aciertos totales) / (Total de predicciones)

Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

---

### ğŸ’¡ Ejemplo con nÃºmeros:

**Matriz de confusiÃ³n del detector de spam:**

```
                LegÃ­timo    Spam
LegÃ­timo           50        5
Spam               10       35

Total emails: 100
```

**CÃ¡lculo:**

```
TP = 35 (spam detectado correctamente)
TN = 50 (legÃ­timo detectado correctamente)
FP = 10 (falsa alarma)
FN = 5 (spam no detectado)

Accuracy = (TP + TN) / Total
Accuracy = (35 + 50) / 100
Accuracy = 85 / 100
Accuracy = 0.85 = 85%
```

**InterpretaciÃ³n:**
"El modelo acierta el 85% de las veces"

---

### âœ… Ventajas de Accuracy:

âœ… **SÃºper fÃ¡cil de entender:** "Acierto X% de las veces"
âœ… **Intuitivo para explicar a no tÃ©cnicos**
âœ… **MÃ©trica mÃ¡s comÃºn y popular**

---

### âš ï¸ PROBLEMA GRAVE con Accuracy: Clases desbalanceadas

**Ejemplo: Detector de fraude**

```
Tienes 1000 transacciones:
- 950 son legÃ­timas (95%)
- 50 son fraude (5%)
```

**Modelo TONTO que siempre predice "legÃ­timo":**

```
                LegÃ­timo    Fraude
LegÃ­timo          950        50
Fraude             0         0

Accuracy = (950 + 0) / 1000 = 95%
```

**Â¡95% de accuracy! Â¿Es bueno?**
âŒ **NO!** El modelo NUNCA detecta fraude
âŒ Es inÃºtil, pero tiene alta accuracy

**Por eso necesitamos Precision y Recall.**

---

## 2ï¸âƒ£ PRECISION (PrecisiÃ³n)

### ğŸ“Š Â¿QuÃ© mide?

**"De todo lo que dije que era POSITIVO, Â¿cuÃ¡nto realmente lo era?"**

**Pregunta:** "Â¿QuÃ© tan confiable soy cuando digo SÃ?"

**FÃ³rmula en palabras:**

```
Precision = TP / (TP + FP)
Precision = Verdaderos positivos / Todos los que dije positivos
```

---

### ğŸ’¡ Ejemplo con spam:

```
Predije "SPAM" para 45 emails:
- 35 SÃ eran spam (TP)
- 10 NO eran spam (FP) â† falsa alarma

Precision = 35 / (35 + 10)
Precision = 35 / 45
Precision = 0.78 = 78%
```

**InterpretaciÃ³n:**
"Cuando digo que es spam, tengo razÃ³n el 78% de las veces"
"22% de las veces bloqueo emails legÃ­timos" â† Â¡problema!

---

### ğŸ¯ Â¿CuÃ¡ndo es importante ALTA Precision?

**Cuando los FALSOS POSITIVOS son MUY costosos:**

**Ejemplo 1: Email spam**

- FP = Email importante va a spam
- Consecuencia: Pierdes informaciÃ³n crÃ­tica
- SoluciÃ³n: Precision alta (mejor dejar pasar spam que bloquear importante)

**Ejemplo 2: RecomendaciÃ³n de pelÃ­culas**

- FP = Recomendar pelÃ­cula que no le gustarÃ¡
- Consecuencia: Usuario molesto, pierde confianza
- SoluciÃ³n: Precision alta (solo recomendar si estÃ¡s seguro)

**Ejemplo 3: Publicidad**

- FP = Mostrar anuncio a persona no interesada
- Consecuencia: Gastas dinero sin retorno
- SoluciÃ³n: Precision alta (solo mostrar a interesados)

---

## 3ï¸âƒ£ RECALL (Exhaustividad / Sensibilidad)

### ğŸ“Š Â¿QuÃ© mide?

**"De todo lo que ERA positivo, Â¿cuÃ¡nto logrÃ© detectar?"**

**Pregunta:** "Â¿QuÃ© tan bueno soy para NO dejar escapar cosas?"

**FÃ³rmula en palabras:**

```
Recall = TP / (TP + FN)
Recall = Verdaderos positivos / Todos los positivos reales
```

---

### ğŸ’¡ Ejemplo con spam:

```
HabÃ­a 40 emails de spam en total:
- 35 los detectÃ© como spam (TP)
- 5 pasaron como legÃ­timos (FN) â† se me escaparon

Recall = 35 / (35 + 5)
Recall = 35 / 40
Recall = 0.875 = 87.5%
```

**InterpretaciÃ³n:**
"Detecto el 87.5% del spam que existe"
"El 12.5% del spam se me escapa a la bandeja principal" â† problema

---

### ğŸ¯ Â¿CuÃ¡ndo es importante ALTO Recall?

**Cuando los FALSOS NEGATIVOS son MUY peligrosos:**

**Ejemplo 1: Detector de cÃ¡ncer**

- FN = Decir "estÃ¡ sano" cuando tiene cÃ¡ncer
- Consecuencia: Paciente muere
- SoluciÃ³n: Recall altÃ­simo (mejor 10 falsas alarmas que perder 1 caso real)

**Ejemplo 2: Detector de fraude bancario**

- FN = No detectar fraude real
- Consecuencia: Pierdes millones de euros
- SoluciÃ³n: Recall alto (mejor bloquear transacciones legÃ­timas que dejar pasar fraude)

**Ejemplo 3: Seguridad aeropuerto**

- FN = No detectar arma/bomba
- Consecuencia: Desastre
- SoluciÃ³n: Recall 99.99% (mejor molestar pasajeros que dejar pasar amenaza)

---

### ğŸ”¥ PRECISION vs RECALL: El trade-off

**El dilema:**

- â†‘ Precision â†’ â†“ Recall
- â†‘ Recall â†’ â†“ Precision

**No puedes tener ambos perfectos simultÃ¡neamente**

---

### ğŸ“Š VisualizaciÃ³n del trade-off:

```
MODO ESTRICTO (Alta Precision):
"Solo digo SÃ si estoy 99% seguro"
â†’ Alta Precision (cuando digo SÃ, casi siempre acierto)
â†’ Baja Recall (me pierdo muchos casos porque soy muy conservador)

Ejemplo: Email spam
Bloqueo solo spam OBVIO â†’ Pocos falsos positivos (FP bajo)
Pero mucho spam pasa â†’ Muchos falsos negativos (FN alto)
```

```
MODO AGRESIVO (Alto Recall):
"Digo SÃ ante la mÃ­nima sospecha"
â†’ Alta Recall (atrapo casi todos los positivos)
â†’ Baja Precision (muchos falsos positivos)

Ejemplo: Detector de cÃ¡ncer
Ante mÃ­nima duda â†’ "posible cÃ¡ncer" (no quiero perderme ninguno)
Detecto todos los casos â†’ Muchas falsas alarmas (FP alto)
```

---

### ğŸ¯ Â¿CuÃ¡l priorizar?

| SituaciÃ³n                 | Prioriza      | Por quÃ©                                                        |
| ------------------------- | ------------- | -------------------------------------------------------------- |
| Email spam                | **Precision** | Peor perder email importante que recibir spam                  |
| Detector de cÃ¡ncer        | **Recall**    | Peor no detectar cÃ¡ncer que falsa alarma                       |
| Detector de fraude        | **Recall**    | Peor perder dinero que bloquear transacciÃ³n legÃ­tima           |
| Recomendador de productos | **Precision** | Peor recomendar mal que no recomendar                          |
| Seguridad aeropuerto      | **Recall**    | Peor dejar pasar amenaza que revisar demÃ¡s                     |
| BÃºsqueda en Google        | **Balance**   | Quieres resultados relevantes (precision) y completos (recall) |

---

## 4ï¸âƒ£ F1-SCORE

### ğŸ“Š Â¿QuÃ© mide?

**"El balance perfecto entre Precision y Recall"**

**Es la media armÃ³nica de Precision y Recall**

**FÃ³rmula en palabras:**

```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

---

### ğŸ’¡ Â¿Por quÃ© F1-Score?

**Problema:**

- Modelo A: Precision = 90%, Recall = 50%
- Modelo B: Precision = 60%, Recall = 95%
- Â¿CuÃ¡l es mejor?

**Promedio simple:**

- Modelo A: (90 + 50) / 2 = 70%
- Modelo B: (60 + 95) / 2 = 77.5%

âŒ Pero promedio simple no captura el balance

**F1-Score (media armÃ³nica):**

- Modelo A: F1 = 2Ã—(90Ã—50)/(90+50) = 64.3%
- Modelo B: F1 = 2Ã—(60Ã—95)/(60+95) = 73.5%

âœ… F1 penaliza desequilibrios entre precision y recall

---

### ğŸ¯ Â¿CuÃ¡ndo usar F1-Score?

âœ… **Cuando necesitas balance entre Precision y Recall**
âœ… **Cuando las clases estÃ¡n desbalanceadas**
âœ… **Como mÃ©trica Ãºnica para comparar modelos**
âœ… **Cuando no puedes decidir si priorizar precision o recall**

---

### ğŸ“Š InterpretaciÃ³n de F1:

| F1-Score      | InterpretaciÃ³n               |
| ------------- | ---------------------------- |
| **0.9 - 1.0** | Excelente balance â­â­â­â­â­ |
| **0.7 - 0.9** | Buen balance â­â­â­â­        |
| **0.5 - 0.7** | Balance moderado â­â­â­      |
| **< 0.5**     | Balance pobre â­â­           |

---

## ğŸ“Š TABLA COMPARATIVA: Las 4 mÃ©tricas

| MÃ©trica       | Â¿QuÃ© mide?                 | FÃ³rmula       | CuÃ¡ndo usarla      | Rango |
| ------------- | -------------------------- | ------------- | ------------------ | ----- |
| **Accuracy**  | % de aciertos totales      | (TP+TN)/Total | Clases balanceadas | 0 a 1 |
| **Precision** | Â¿Confiable cuando digo SÃ? | TP/(TP+FP)    | FP son costosos    | 0 a 1 |
| **Recall**    | Â¿Detecto todos los SÃ?     | TP/(TP+FN)    | FN son peligrosos  | 0 a 1 |
| **F1-Score**  | Balance P y R              | 2(PÃ—R)/(P+R)  | Necesitas balance  | 0 a 1 |

**Para todas: MÃ¡s cerca de 1 = mejor**

---

## ğŸ“ EJEMPLO COMPLETO: Detector de fraude

### Caso: Evaluar detector de fraude bancario

**Evaluaste 1000 transacciones:**

```
MATRIZ DE CONFUSIÃ“N

                  LegÃ­timo    Fraude
LegÃ­timo             940        10
Fraude                30        20

Total: 1000 transacciones
- 970 legÃ­timas
- 30 fraude
```

**Identificar los valores:**

- TP = 20 (fraude detectado correctamente)
- TN = 940 (legÃ­timo detectado correctamente)
- FP = 30 (bloqueÃ© transacciÃ³n legÃ­tima - molestia cliente)
- FN = 10 (fraude no detectado - perdÃ­ dinero)

---

### ğŸ“Š Calcular todas las mÃ©tricas:

**1. Accuracy:**

```
Accuracy = (TP + TN) / Total
Accuracy = (20 + 940) / 1000
Accuracy = 960 / 1000 = 0.96 = 96%
```

âœ… "Acierto el 96% de las veces"

---

**2. Precision:**

```
Precision = TP / (TP + FP)
Precision = 20 / (20 + 30)
Precision = 20 / 50 = 0.40 = 40%
```

âš ï¸ "Cuando digo que es fraude, solo acierto el 40% de las veces"
âš ï¸ "Bloqueo muchas transacciones legÃ­timas (60%)"

---

**3. Recall:**

```
Recall = TP / (TP + FN)
Recall = 20 / (20 + 10)
Recall = 20 / 30 = 0.67 = 67%
```

âš ï¸ "Detecto solo el 67% del fraude real"
âš ï¸ "El 33% del fraude se me escapa (pierdo dinero)"

---

**4. F1-Score:**

```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
F1 = 2 Ã— (0.40 Ã— 0.67) / (0.40 + 0.67)
F1 = 2 Ã— 0.268 / 1.07
F1 = 0.50 = 50%
```

âŒ "El balance es pobre"

---

### ğŸ“‹ EvaluaciÃ³n completa:

| MÃ©trica   | Valor | InterpretaciÃ³n                 | En definicion de fraude                                                                                      |
| --------- | ----- | ------------------------------ | ------------------------------------------------------------------------------------------------------------ |
| Accuracy  | 96%   | âœ… Parece bueno, pero engaÃ±oso | Cuando veo que la mayoria NO son fraude                                                                      |
| Precision | 40%   | âŒ Molesto muchos clientes     | Porcentaje bajo. Cuando digo, que si eran fraude, pero NO lo eran. Alto porcentaje de precision da muchos FN |
| Recall    | 67%   | âš ï¸ Pierdo 1 de cada 3 fraudes  | Porcentaje medio. Cuando digo, que no eran fraude, pero SI lo eran. Alto Porcentaje de Recall da muchos FP   |
| F1-Score  | 50%   | âŒ Modelo pobre en general     | porcentaje medio. Balance del fraude mediocre                                                                |

**ConclusiÃ³n:**

- Alta accuracy engaÃ±a (porque 97% son legÃ­timos)
- El modelo es MALO para detectar fraude
- Necesita mejorar tanto precision como recall

---

## ğŸ“ PREGUNTAS TIPO EXAMEN

### Pregunta 1:

\*\*Un modelo de clasificaciÃ³n tiene estos resultados:

- TP = 80
- TN = 800
- FP = 20
- FN = 100

Â¿CuÃ¡l es la Accuracy del modelo?\*\*

A) 80%
B) 88% âœ…
C) 44%
D) 50%

**CÃ¡lculo:**

```
Accuracy = (TP + TN) / Total
Accuracy = (80 + 800) / (80 + 800 + 20 + 100)
Accuracy = 880 / 1000 = 0.88 = 88%
```

---

### Pregunta 2:

**En un detector de enfermedades raras, Â¿quÃ© mÃ©trica es MÃS importante priorizar?**

A) Accuracy
B) Precision
C) Recall âœ…
D) F1-Score

**Por quÃ© C:** En enfermedades, los falsos negativos (no detectar enfermedad real) son peligrosos. Recall mide quÃ© tan bien detectas TODOS los casos positivos.

---

### Pregunta 3:

**Â¿QuÃ© representa un Falso Positivo (FP) en un detector de spam?**

A) Email spam correctamente identificado como spam
B) Email legÃ­timo correctamente identificado como legÃ­timo
C) Email legÃ­timo incorrectamente identificado como spam âœ…
D) Email spam incorrectamente identificado como legÃ­timo

**Por quÃ© C:** Falso Positivo = dijiste "positivo (spam)" pero era falso (legÃ­timo).

---

### Pregunta 4:

**Tienes un modelo con Precision = 0.90 y Recall = 0.50. Â¿QuÃ© significa?**

A) El modelo es excelente
B) El modelo es muy confiable cuando predice positivo, pero se pierde muchos casos positivos âœ…
C) El modelo detecta todos los positivos pero tiene muchos falsos positivos
D) El modelo tiene bajo rendimiento en general

**Por quÃ© B:** Alta precision = confiable cuando dice SÃ. Bajo recall = se pierde muchos casos (muchos FN).

---

### Pregunta 5:

**Â¿CuÃ¡l es la principal limitaciÃ³n de usar solo Accuracy para evaluar un clasificador?**

A) Es difÃ­cil de calcular
B) No funciona bien con clases desbalanceadas âœ…
C) No estÃ¡ disponible en Azure ML
D) Solo funciona para clasificaciÃ³n binaria

**Por quÃ© B:** Con clases desbalanceadas (ej: 95% clase A, 5% clase B), un modelo que siempre predice clase A tendrÃ¡ 95% accuracy pero es inÃºtil.

---

## âœ… TAREAS DE HOY (MiÃ©rcoles)

### 1. Microsoft Learn (45 min)

**MÃ³dulos a completar:**

- **"CreaciÃ³n de modelos de clasificaciÃ³n"**
- **"Entrenamiento y evaluaciÃ³n de modelos de clasificaciÃ³n"**

Link: https://learn.microsoft.com/es-es/training/modules/train-evaluate-classification-models/

---

### 2. Ejercicio: Calcular mÃ©tricas (20 min)

**Para cada matriz de confusiÃ³n, calcula las 4 mÃ©tricas:**

**Escenario 1: Detector de productos defectuosos**

```
                Bueno    Defectuoso
Bueno            850         50
Defectuoso        20         80

Total: 1000 productos
```

Calcula:

- TP = \_\_\_ 80
- TN = \_\_\_ 850
- FP = \_\_\_ 20
- FN = \_\_\_ 50
- Accuracy = \_\_\_ 0.93
- Precision = \_\_\_ 0.8
- Recall = \_\_\_ 0.61
- Â¿Es un buen modelo? \_\_\_

---

**Escenario 2: Clasificador de emociones (positivo/negativo)**

```
               Negativo   Positivo
Negativo          300        50
Positivo          100       550

Total: 1000 reseÃ±as
```

Calcula:

- TP = \_\_\_
- TN = \_\_\_
- FP = \_\_\_
- FN = \_\_\_
- Accuracy = \_\_\_
- Precision = \_\_\_
- Recall = \_\_\_

---

**Escenario 3: Detector mÃ©dico de diabetes**

```
               Sano    DiabÃ©tico
Sano           920        10
DiabÃ©tico       40        30

Total: 1000 pacientes
```

Calcula todas las mÃ©tricas y responde:

- Â¿QuÃ© mÃ©trica es mÃ¡s preocupante? \_\_\_
- Â¿Por quÃ©? \_\_\_
- Â¿QuÃ© deberÃ­a mejorar este modelo? \_\_\_

---

### 3. Crea Flashcards (15 min)

**Crea estas 15 tarjetas:**

**Tarjeta 1:**

- Frente: "Â¿QuÃ© mide Accuracy?"
- AtrÃ¡s: "Porcentaje de predicciones correctas. (TP+TN)/Total"

**Tarjeta 2:**

- Frente: "Â¿QuÃ© mide Precision?"
- AtrÃ¡s: "De lo que dije positivo, Â¿cuÃ¡nto era realmente positivo? TP/(TP+FP)"

**Tarjeta 3:**

- Frente: "Â¿QuÃ© mide Recall?"
- AtrÃ¡s: "De todo lo que ERA positivo, Â¿cuÃ¡nto detectÃ©? TP/(TP+FN)"

**Tarjeta 4:**

- Frente: "Â¿QuÃ© es True Positive (TP)?"
- AtrÃ¡s: "Predije positivo Y era positivo. âœ… Â¡Acierto!"

**Tarjeta 5:**

- Frente: "Â¿QuÃ© es False Positive (FP)?"
- AtrÃ¡s: "Predije positivo pero era negativo. âŒ Falsa alarma"

**Tarjeta 6:**

- Frente: "Â¿QuÃ© es False Negative (FN)?"
- AtrÃ¡s: "Predije negativo pero era positivo. âŒ Se me escapÃ³"

**Tarjeta 7:**

- Frente: "Â¿QuÃ© es True Negative (TN)?"
- AtrÃ¡s: "Predije negativo Y era negativo. âœ… Â¡Acierto!"

**Tarjeta 8:**

- Frente: "Â¿CuÃ¡ndo priorizar Precision?"
- AtrÃ¡s: "Cuando los FALSOS POSITIVOS son muy costosos. Ej: email spam, recomendaciones"

**Tarjeta 9:**

- Frente: "Â¿CuÃ¡ndo priorizar Recall?"
- AtrÃ¡s: "Cuando los FALSOS NEGATIVOS son peligrosos. Ej: detector de cÃ¡ncer, fraude"

**Tarjeta 10:**

- Frente: "Â¿QuÃ© mide F1-Score?"
- AtrÃ¡s: "Balance entre Precision y Recall. Media armÃ³nica de ambas."

**Tarjeta 11:**

- Frente: "Problema con Accuracy en clases desbalanceadas"
- AtrÃ¡s: "Un modelo tonto que predice siempre la clase mayoritaria tendrÃ¡ alta accuracy pero es inÃºtil"

**Tarjeta 12:**

- Frente: "Trade-off Precision vs Recall"
- AtrÃ¡s: "Al aumentar Precision, baja Recall y viceversa. No puedes maximizar ambas simultÃ¡neamente"

**Tarjeta 13:**

- Frente: "Ejemplo de Falso Positivo peligroso"
- AtrÃ¡s: "Detector de spam marca email importante como spam â†’ pierdes info crÃ­tica"

**Tarjeta 14:**

- Frente: "Ejemplo de Falso Negativo peligroso"
- AtrÃ¡s: "Detector mÃ©dico dice 'no hay cÃ¡ncer' pero sÃ­ lo hay â†’ paciente no recibe tratamiento"

**Tarjeta 15:**

- Frente: "Diferencia clave: RegresiÃ³n vs ClasificaciÃ³n mÃ©tricas"
- AtrÃ¡s: "RegresiÃ³n: MAE, RMSE, RÂ² (para nÃºmeros). ClasificaciÃ³n: Accuracy, Precision, Recall, F1 (para categorÃ­as)"

---

## ğŸ“ CONCEPTOS CLAVE DEL MIÃ‰RCOLES

**Memoriza:**

- ClasificaciÃ³n predice categorÃ­as (no nÃºmeros)
- Matriz de confusiÃ³n: TP, TN, FP, FN
- Accuracy = aciertos totales / total
- Precision = confiable cuando digo SÃ
- Recall = detecto todos los SÃ reales
- F1 = balance entre Precision y Recall
- FP = falsa alarma, FN = se me escapÃ³
- Prioriza Precision cuando FP sean costosos
- Prioriza Recall cuando FN sean peligrosos

---

## âœ… CHECKLIST MIÃ‰RCOLES

- [ ] Entiendo quÃ© es clasificaciÃ³n vs regresiÃ³n
- [ ] Domino la matriz de confusiÃ³n (TP, TN, FP, FN)
- [ ] SÃ© quÃ© miden Accuracy, Precision, Recall, F1
- [ ] Puedo calcular mÃ©tricas de una matriz de confusiÃ³n
- [ ] Entiendo cuÃ¡ndo priorizar Precision vs Recall
- [ ] SÃ© el problema de Accuracy con clases desbalanceadas
- [ ] CompletÃ© los 3 ejercicios de cÃ¡lculo
- [ ] CreÃ© 15 flashcards nuevas
- [ ] RepasÃ© flashcards de Lunes y Martes (10 min)
- [ ] Puedo explicar las mÃ©tricas en voz alta

---

## ğŸ“š RESPUESTAS A LOS EJERCICIOS

### Escenario 1: Detector de productos defectuosos

```
                Bueno    Defectuoso
Bueno            850         50
Defectuoso        20         80
```

**Identificar valores:**

- **TP = 80** (defectuosos detectados correctamente)
- **TN = 850** (buenos detectados correctamente)
- **FP = 20** (buenos marcados como defectuosos - desperdicio)
- **FN = 50** (defectuosos que pasaron como buenos - PELIGROSO)

**CÃ¡lculos:**

```
Accuracy = (80 + 850) / 1000 = 930 / 1000 = 0.93 = 93%

Precision = 80 / (80 + 20) = 80 / 100 = 0.80 = 80%

Recall = 80 / (80 + 50) = 80 / 130 = 0.615 = 61.5%

F1 = 2 Ã— (0.80 Ã— 0.615) / (0.80 + 0.615) = 0.695 = 69.5%
```

**EvaluaciÃ³n:**

- âœ… Accuracy alta (93%) - pero puede engaÃ±ar
- âœ… Precision aceptable (80%) - cuando marca defectuoso, suele acertar
- âš ï¸ Recall bajo (61.5%) - se pierde muchos defectuosos (FN = 50)
- **Problema:** 50 productos defectuosos llegan a clientes â†’ quejas, devoluciones
- **Necesita mejorar:** RECALL (detectar mÃ¡s defectuosos)

---

### Escenario 2: Clasificador de emociones

```
               Negativo   Positivo
Negativo          300        50
Positivo          100       550
```

**Identificar valores:**

- **TP = 550** (positivos detectados correctamente)
- **TN = 300** (negativos detectados correctamente)
- **FP = 100** (negativos marcados como positivos)
- **FN = 50** (positivos marcados como negativos)

**CÃ¡lculos:**

```
Accuracy = (550 + 300) / 1000 = 850 / 1000 = 0.85 = 85%

Precision = 550 / (550 + 100) = 550 / 650 = 0.846 = 84.6%

Recall = 550 / (550 + 50) = 550 / 600 = 0.917 = 91.7%

F1 = 2 Ã— (0.846 Ã— 0.917) / (0.846 + 0.917) = 0.880 = 88%
```

**EvaluaciÃ³n:**

- âœ… Accuracy buena (85%)
- âœ… Precision buena (84.6%)
- âœ…âœ… Recall excelente (91.7%)
- âœ… F1 muy bueno (88%)
- **ConclusiÃ³n:** Modelo BUENO en general, buen balance

---

### Escenario 3: Detector mÃ©dico de diabetes

```
               Sano    DiabÃ©tico
Sano           920        10
DiabÃ©tico       40        30
```

**Identificar valores:**

- **TP = 30** (diabÃ©ticos detectados)
- **TN = 920** (sanos detectados)
- **FP = 40** (sanos marcados como diabÃ©ticos - falsa alarma)
- **FN = 10** (diabÃ©ticos marcados como sanos - PELIGROSÃSIMO)

**CÃ¡lculos:**

```
Accuracy = (30 + 920) / 1000 = 950 / 1000 = 0.95 = 95%

Precision = 30 / (30 + 40) = 30 / 70 = 0.429 = 42.9%

Recall = 30 / (30 + 10) = 30 / 40 = 0.75 = 75%

F1 = 2 Ã— (0.429 Ã— 0.75) / (0.429 + 0.75) = 0.545 = 54.5%
```

**EvaluaciÃ³n y anÃ¡lisis:**

- âœ… Accuracy muy alta (95%) - PERO ES ENGAÃ‘OSA
- âŒ Precision baja (42.9%) - muchas falsas alarmas (40 sanos diagnosticados)
- âš ï¸ Recall moderado (75%) - pero 10 diabÃ©ticos no detectados
- âŒ F1 bajo (54.5%) - modelo pobre en general

**Â¿QuÃ© mÃ©trica es mÃ¡s preocupante?**
**RECALL (75%)**

**Â¿Por quÃ©?**

- FN = 10 significa que 10 personas con diabetes NO fueron detectadas
- Estas personas NO recibirÃ¡n tratamiento
- Consecuencia: Complicaciones graves, posible muerte
- En medicina, los FN son CRÃTICOS

**Â¿QuÃ© deberÃ­a mejorar?**

- **PRIORIDAD 1:** Aumentar RECALL (detectar mÃ¡s diabÃ©ticos)
- Mejor tener 100 falsas alarmas que perder 1 caso real
- DespuÃ©s de mejorar recall, trabajar en precision para reducir falsas alarmas

---

## ğŸ¯ COMPARACIÃ“N FINAL: RegresiÃ³n vs ClasificaciÃ³n

### ğŸ“Š Tabla resumen completa:

| Aspecto                       | REGRESIÃ“N                               | CLASIFICACIÃ“N                     |
| ----------------------------- | --------------------------------------- | --------------------------------- |
| **Predice**                   | NÃºmeros continuos                       | CategorÃ­as discretas              |
| **Ejemplo output**            | 150,000â‚¬, 25.5Â°C                        | "Spam", "Gato", "SÃ­"              |
| **Pregunta**                  | "Â¿CuÃ¡nto?"                              | "Â¿CuÃ¡l?"                          |
| **MÃ©tricas principales**      | MAE, RMSE, RÂ²                           | Accuracy, Precision, Recall, F1   |
| **Mejor valor mÃ©tricas**      | MAE/RMSE: 0 (bajo), RÂ²: 1 (alto)        | Todas: 1 (alto)                   |
| **Concepto clave evaluaciÃ³n** | Error numÃ©rico y variabilidad explicada | Matriz de confusiÃ³n               |
| **Problema comÃºn**            | Overfitting                             | Clases desbalanceadas             |
| **Ejemplos**                  | Precio casas, temperatura, ventas       | Spam/no spam, fraude, diagnÃ³stico |

---

## ğŸ“ EJERCICIO MENTAL: Identifica el tipo

**Para cada problema, Â¿es RegresiÃ³n o ClasificaciÃ³n?**

1. Predecir si un cliente comprarÃ¡ o no
   - **ClasificaciÃ³n** (SÃ­/No)

2. Estimar cuÃ¡nto gastarÃ¡ un cliente en su prÃ³xima compra
   - **RegresiÃ³n** (cantidad en â‚¬)

3. Identificar si una imagen muestra un perro, gato o pÃ¡jaro
   - **ClasificaciÃ³n** (3 categorÃ­as)

4. Predecir cuÃ¡ntos dÃ­as de hospitalizaciÃ³n necesitarÃ¡ un paciente
   - **RegresiÃ³n** (nÃºmero de dÃ­as)

5. Determinar el nivel de satisfacciÃ³n: bajo, medio, alto
   - **ClasificaciÃ³n** (3 categorÃ­as)

6. Estimar el precio de un coche usado
   - **RegresiÃ³n** (precio en â‚¬)

7. Clasificar emails en trabajo, personal, promociones, spam
   - **ClasificaciÃ³n** (4 categorÃ­as)

8. Predecir la temperatura mÃ¡xima de maÃ±ana
   - **RegresiÃ³n** (temperatura en Â°C)

---

## ğŸ“‹ CHEAT SHEET PARA IMPRIMIR

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        MÃ‰TRICAS DE CLASIFICACIÃ“N - CHEAT SHEET          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ MATRIZ DE CONFUSIÃ“N                                     â•‘
â•‘                      REALIDAD                            â•‘
â•‘               Positivo      Negativo                     â•‘
â•‘  PREDICCIÃ“N  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â•‘
â•‘  Positivo    â”‚    TP    â”‚    FP    â”‚                    â•‘
â•‘              â”‚  âœ…âœ…    â”‚  âŒ      â”‚                    â•‘
â•‘              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                    â•‘
â•‘  Negativo    â”‚    FN    â”‚    TN    â”‚                    â•‘
â•‘              â”‚  âŒ      â”‚  âœ…âœ…    â”‚                    â•‘
â•‘              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ACCURACY (Exactitud)                                     â•‘
â•‘ â€¢ FÃ³rmula: (TP + TN) / Total                            â•‘
â•‘ â€¢ QuÃ© mide: % de aciertos totales                       â•‘
â•‘ â€¢ CuÃ¡ndo: Clases balanceadas                            â•‘
â•‘ â€¢ âš ï¸ EngaÃ±osa con clases desbalanceadas                 â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ PRECISION (PrecisiÃ³n)                                    â•‘
â•‘ â€¢ FÃ³rmula: TP / (TP + FP)                               â•‘
â•‘ â€¢ QuÃ© mide: Confiable cuando digo SÃ                    â•‘
â•‘ â€¢ Pregunta: "De lo que dije positivo, Â¿cuÃ¡nto lo era?" â•‘
â•‘ â€¢ Prioriza: Cuando FP son costosos                      â•‘
â•‘ â€¢ Ejemplo: Email spam, recomendaciones                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ RECALL (Exhaustividad/Sensibilidad)                     â•‘
â•‘ â€¢ FÃ³rmula: TP / (TP + FN)                               â•‘
â•‘ â€¢ QuÃ© mide: Detecto todos los positivos                 â•‘
â•‘ â€¢ Pregunta: "De todo lo positivo, Â¿cuÃ¡nto detectÃ©?"    â•‘
â•‘ â€¢ Prioriza: Cuando FN son peligrosos                    â•‘
â•‘ â€¢ Ejemplo: CÃ¡ncer, fraude, seguridad                    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ F1-SCORE (Balance)                                       â•‘
â•‘ â€¢ FÃ³rmula: 2Ã—(PÃ—R)/(P+R)                                â•‘
â•‘ â€¢ QuÃ© mide: Balance entre Precision y Recall            â•‘
â•‘ â€¢ CuÃ¡ndo: Necesitas balance, clases desbalanceadas     â•‘
â•‘ â€¢ Media armÃ³nica (penaliza desequilibrios)              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ REGLAS RÃPIDAS                                           â•‘
â•‘ â€¢ TP = Acierto positivo âœ…âœ…                             â•‘
â•‘ â€¢ TN = Acierto negativo âœ…âœ…                             â•‘
â•‘ â€¢ FP = Falsa alarma âŒ (dije SÃ, era NO)                â•‘
â•‘ â€¢ FN = Se me escapÃ³ âŒ (dije NO, era SÃ)                â•‘
â•‘                                                          â•‘
â•‘ â€¢ â†‘ Precision â†’ â†“ Recall (trade-off)                    â•‘
â•‘ â€¢ Accuracy engaÃ±a en clases desbalanceadas              â•‘
â•‘ â€¢ FN peligrosos â†’ prioriza Recall                       â•‘
â•‘ â€¢ FP costosos â†’ prioriza Precision                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸŠ Â¡EXCELENTE TRABAJO EN EL MIÃ‰RCOLES!

**Lo que has logrado hoy:**

âœ… **Dominas clasificaciÃ³n en profundidad**

- ClasificaciÃ³n binaria y multiclase
- Diferencia con regresiÃ³n

âœ… **Entiendes la matriz de confusiÃ³n**

- TP, TN, FP, FN
- QuÃ© significa cada uno

âœ… **Dominas las 4 mÃ©tricas clave**

- Accuracy, Precision, Recall, F1-Score
- CuÃ¡ndo usar cada una
- CÃ³mo calcularlas

âœ… **Entiendes el trade-off Precision vs Recall**

- Por quÃ© no puedes maximizar ambos
- CuÃ¡ndo priorizar cada uno

âœ… **Puedes evaluar modelos de clasificaciÃ³n**

- Interpretar mÃ©tricas en contexto
- Identificar problemas
- Sugerir mejoras

---

## ğŸ“… MAÃ‘ANA (Jueves):

**Tema:** Azure Machine Learning en detalle

- QuÃ© es Azure ML workspace
- Componentes principales
- Designer (herramienta visual)
- CÃ³mo desplegar modelos
- Azure ML vs otros servicios

**PrepÃ¡rate para:** Conectar toda la teorÃ­a con la prÃ¡ctica en Azure

---

## ğŸ’¡ CONEXIÃ“N CON LO QUE VIENE

**Esta semana:**

- âœ… Lunes: Tipos de ML (supervisado, no supervisado, refuerzo)
- âœ… Martes: RegresiÃ³n y mÃ©tricas (MAE, RMSE, RÂ²)
- âœ… MiÃ©rcoles: ClasificaciÃ³n y mÃ©tricas (HOY)
- ğŸ“… Jueves: Azure ML workspace
- ğŸ“… Viernes: Automated ML (AutoML)
- ğŸ“… SÃ¡bado: LAB - Crear tu primer modelo real

**El sÃ¡bado pondrÃ¡s en prÃ¡ctica TODO esto:**

- CrearÃ¡s un modelo de clasificaciÃ³n o regresiÃ³n
- VerÃ¡s las mÃ©tricas en Azure ML
- InterpretarÃ¡s los resultados
- EntenderÃ¡s quÃ© significa cada nÃºmero

---

## ğŸ¯ MINI QUIZ FINAL (5 min)

**Responde mentalmente (sin mirar):**

1. Â¿QuÃ© es un Falso Positivo?
2. Â¿QuÃ© mÃ©trica usarÃ­as en un detector de cÃ¡ncer?
3. Si Precision = 0.90 y Recall = 0.40, Â¿quÃ© problema hay?
4. Â¿Por quÃ© Accuracy puede engaÃ±ar?
5. Â¿QuÃ© mide F1-Score?

**Respuestas:**

1. Dije "positivo" pero era "negativo" - falsa alarma
2. RECALL (no quiero perderme ningÃºn caso)
3. Modelo muy conservador, se pierde muchos casos (bajo recall)
4. Con clases desbalanceadas, un modelo tonto puede tener alta accuracy
5. Balance entre Precision y Recall

**Si acertaste 4-5:** Â¡Perfecto! Listo para maÃ±ana
**Si acertaste 2-3:** Repasa matriz de confusiÃ³n y mÃ©tricas
**Si acertaste 0-1:** Repasa toda la secciÃ³n de mÃ©tricas 15 min

---

## ğŸ“– RECURSOS ADICIONALES (Opcional)

**Si quieres profundizar:**

**Videos recomendados (YouTube):**

- "Confusion Matrix explained" - StatQuest
- "Precision and Recall" - explicaciones visuales
- "F1 Score explained" - tutoriales cortos
- "Classification metrics" - comparaciones

**Microsoft Learn:**

- "Train and evaluate classification models"
- "Understand classification metrics"

**DocumentaciÃ³n Azure ML:**

- CÃ³mo interpretar mÃ©tricas en Azure ML
- Matriz de confusiÃ³n en Azure ML Studio

---

## ğŸ’­ REFLEXIÃ“N FINAL DEL DÃA

**Antes de terminar, reflexiona 2 minutos:**

1. Â¿QuÃ© mÃ©trica te pareciÃ³ mÃ¡s Ãºtil? Â¿Por quÃ©?
2. Â¿Puedes pensar en un problema de clasificaciÃ³n en tu trabajo/vida?
3. Â¿QuÃ© priorizarÃ­as: Precision o Recall? Â¿Por quÃ©?

**Ejemplo de reflexiÃ³n:**
"Recall me parece la mÃ¡s crÃ­tica en medicina. En mi trabajo de atenciÃ³n al cliente, podrÃ­amos clasificar tickets por urgencia. PriorizarÃ­a Recall para no perder tickets urgentes..."

---

## ğŸŒ™ ANTES DE DORMIR (5 min)

**Repaso relÃ¡mpago:**

- Cierra los ojos
- Visualiza la matriz de confusiÃ³n: TP, TN, FP, FN
- Recuerda: Accuracy = total aciertos, Precision = confiable, Recall = detecta todos
- Piensa en ejemplos donde FP son peores vs donde FN son peores

**Repasa tus flashcards nuevas 2 veces**

**Duerme bien.** MaÃ±ana conectamos todo con Azure ML. ğŸ˜´

---

## ğŸ“Š PROGRESO SEMANA 2

```
Lunes:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% âœ…
Martes:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% âœ…
MiÃ©rcoles: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100% âœ…
Jueves:    â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0%
Viernes:   â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0%
SÃ¡bado:    â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0%
```

**Horas Semana 2:** 4.5/10 horas completadas (45%) âœ…
**Progreso Total:** 14.5/60 horas (24.2%) ğŸ“ˆ

---

**Â¡Nos vemos maÃ±ana Jueves para Azure Machine Learning workspace!** ğŸš€

**MaÃ±ana aprenderÃ¡s:**

- QuÃ© es Azure ML workspace y sus componentes
- Datasets, experiments, models, endpoints
- Azure ML Designer (visual, sin cÃ³digo)
- Diferencia entre Azure ML y Azure AI Services
- CuÃ¡ndo usar cada uno

**SerÃ¡ el puente entre teorÃ­a y prÃ¡ctica.** ğŸ’ª
