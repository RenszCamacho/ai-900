# AI-900 Certification - Contenido Complementario

## Temas Actualizados Mayo 2025

**Fecha:** Complemento al Mega Repaso  
**Duraci√≥n estimada:** 30-45 minutos  
**Nivel:** Fundamental

---

## üìã Objetivo

Cubrir temas nuevos/actualizados en la gu√≠a oficial de AI-900 (actualizada mayo 2025):

1. Azure AI Foundry (antes Azure AI Studio)
2. Cat√°logo de modelos de Azure AI Foundry
3. Arquitectura Transformer
4. Actualizaciones terminol√≥gicas importantes

---

## 1. Azure AI Foundry (Antes Azure AI Studio)

### ¬øQu√© es Azure AI Foundry?

**Azure AI Foundry** es el nuevo nombre del portal unificado de Microsoft para desarrollo de aplicaciones de IA. Anteriormente conocido como "Azure AI Studio" y "Azure Machine Learning Studio".

**Nombre oficial actual:** Azure AI Foundry

### Componentes Principales

#### 1. Portal Unificado

**Azure AI Foundry Portal** incluye:

- **Projects**: Espacios de trabajo para organizar recursos de IA
- **Hubs**: Colecciones de recursos compartidos entre proyectos
- **Deployments**: Gesti√≥n de modelos desplegados
- **Playground**: Interfaz interactiva para probar modelos
- **Model Catalog**: Cat√°logo de modelos disponibles

#### 2. Capabilities (Capacidades)

**Azure AI Foundry proporciona:**

‚úÖ **Desarrollo unificado:**

- Un solo lugar para todos los servicios de Azure AI
- Interfaz consistente para diferentes tipos de modelos
- Integraci√≥n con VS Code y GitHub

‚úÖ **Model deployment:**

- Despliegue de modelos con pocos clics
- Configuraci√≥n de endpoints
- Escalado autom√°tico
- Monitoreo integrado

‚úÖ **Prompt engineering:**

- Playground interactivo
- Testing de prompts
- Comparaci√≥n de modelos
- Export de c√≥digo (Python, C#, JavaScript)

‚úÖ **Safety & Responsible AI:**

- Content filters configurables
- Evaluaci√≥n de riesgos
- Transparency tools
- Responsible AI dashboard

‚úÖ **Data & Fine-tuning:**

- Subir datasets personalizados
- Fine-tune de modelos
- Evaluaci√≥n de rendimiento
- Gesti√≥n de versiones

### Azure AI Foundry vs Azure OpenAI Studio

**Relaci√≥n:**

- Azure OpenAI Studio est√° **dentro** de Azure AI Foundry
- Azure AI Foundry es m√°s amplio (incluye todos los servicios de Azure AI)
- Azure OpenAI Studio se enfoca espec√≠ficamente en modelos OpenAI

```
Azure AI Foundry (Portal completo)
    ‚îú‚îÄ‚îÄ Azure OpenAI Service
    ‚îÇ   ‚îî‚îÄ‚îÄ GPT-5, GPT-4, DALL-E, Whisper
    ‚îú‚îÄ‚îÄ Azure AI Vision
    ‚îú‚îÄ‚îÄ Azure AI Language
    ‚îú‚îÄ‚îÄ Azure AI Speech
    ‚îú‚îÄ‚îÄ Azure Machine Learning
    ‚îî‚îÄ‚îÄ Model Catalog (otros modelos)
```

### Acceso a Azure AI Foundry

**URL:** https://ai.azure.com

**Requisitos:**

- Suscripci√≥n de Azure
- Permisos apropiados (Contributor o Owner)
- Recursos de Azure AI creados

### Flujo de Trabajo T√≠pico en Azure AI Foundry

**Paso 1: Crear Hub**

```
Azure AI Foundry ‚Üí Create Hub
- Nombre del hub
- Regi√≥n
- Resource group
- Azure OpenAI connection (opcional)
```

**Paso 2: Crear Project**

```
Hub ‚Üí Create Project
- Nombre del proyecto
- Asociado al hub
```

**Paso 3: Seleccionar Modelo**

```
Project ‚Üí Model Catalog
- Explorar modelos disponibles
- Seleccionar modelo (ej: GPT-5)
```

**Paso 4: Desplegar Modelo**

```
Model ‚Üí Deploy
- Nombre del deployment
- Configurar TPM
- Regi√≥n
```

**Paso 5: Probar en Playground**

```
Deployment ‚Üí Open Playground
- Escribir prompts
- Ajustar par√°metros
- Ver resultados
```

**Paso 6: Integrar en Aplicaci√≥n**

```
Playground ‚Üí View Code
- Copiar c√≥digo de ejemplo
- Usar en tu app (Python, C#, JS)
```

---

## 2. Cat√°logo de Modelos de Azure AI Foundry

### ¬øQu√© es el Model Catalog?

El **Model Catalog** (Cat√°logo de Modelos) es una colecci√≥n curada de modelos de IA disponibles en Azure AI Foundry.

### Tipos de Modelos en el Cat√°logo

#### 1. Modelos de Microsoft

**Azure OpenAI models:**

- GPT-5 (y variantes: mini, nano, chat)
- GPT-4 (y variantes: turbo, o)
- GPT-3.5-turbo
- DALL-E 2, DALL-E 3
- Whisper
- text-embedding-ada-002
- text-embedding-3-small/large

**Azure AI models:**

- Phi-3 (Small Language Model de Microsoft)
- Florence (Vision-Language model)
- Orca (Reasoning model)

#### 2. Modelos Open Source

**Meta (Facebook):**

- Llama 2 (7B, 13B, 70B)
- Llama 3 (8B, 70B, 405B)
- Code Llama (programaci√≥n)

**Mistral AI:**

- Mistral 7B
- Mixtral 8x7B
- Mistral Large

**Otros:**

- Cohere Command
- AI21 Jurassic
- Stable Diffusion (im√°genes)
- Falcon

#### 3. Modelos de Socios (Partners)

**Hugging Face:**

- Acceso a miles de modelos de Hugging Face Hub
- Integraci√≥n directa

**NVIDIA:**

- Modelos optimizados para GPUs NVIDIA
- Nemotron

### Caracter√≠sticas del Model Catalog

#### Informaci√≥n de Modelos

Para cada modelo, el cat√°logo proporciona:

**1. Descripci√≥n:**

- Qu√© hace el modelo
- Casos de uso apropiados
- Limitaciones conocidas

**2. Especificaciones t√©cnicas:**

- Tama√±o del modelo (par√°metros)
- Context window
- Input/Output types
- Idiomas soportados

**3. Licencia:**

- MIT, Apache 2.0, Commercial, etc.
- Restricciones de uso

**4. Pricing:**

- Pay-as-you-go
- PTU (Provisioned Throughput Units)
- Free tier (algunos modelos)

**5. Disponibilidad:**

- Regiones donde est√° disponible
- Requisitos de acceso

#### Deployment Options

**Deployment types disponibles:**

**1. Serverless API:**

- Sin infraestructura que gestionar
- Pay-per-use
- Escalado autom√°tico
- Ideal para: Experimentaci√≥n, desarrollo

**2. Managed Online Endpoints:**

- Control sobre compute
- Dedicated resources
- SLA garantizado
- Ideal para: Producci√≥n

**3. Batch Endpoints:**

- Procesamiento por lotes
- M√°s econ√≥mico para grandes vol√∫menes
- No real-time
- Ideal para: Procesamiento masivo

### C√≥mo Usar el Model Catalog

#### Explorar Modelos

**Filtros disponibles:**

- **Task type**: Text generation, Vision, Speech, etc.
- **Publisher**: Microsoft, Meta, Mistral, etc.
- **License**: Open source, Commercial
- **Deployment option**: Serverless, Managed

**Ejemplo de b√∫squeda:**

```
Task: "Text generation"
Publisher: "Meta"
License: "Open source"

Resultado: Llama 3 models
```

#### Comparar Modelos

**M√©tricas de comparaci√≥n:**

- Performance (benchmarks)
- Context window
- Cost per token
- Latency
- Capabilities

**Ejemplo:**

```
GPT-4 vs Llama 3 70B:
- Performance: GPT-4 superior en razonamiento
- Context window: GPT-4 128K, Llama 3 8K
- Cost: Llama 3 m√°s econ√≥mico
- Latency: Similar
```

#### Desplegar desde el Cat√°logo

**Pasos:**

1. Model Catalog ‚Üí Seleccionar modelo
2. "Deploy" button
3. Elegir deployment type (Serverless/Managed)
4. Configurar:
   - Deployment name
   - Region
   - Compute size (si Managed)
5. Deploy
6. Obtener endpoint y key

---

## 3. Arquitectura Transformer

### ¬øQu√© es la Arquitectura Transformer?

**Transformer** es la arquitectura de red neuronal que revolucion√≥ el procesamiento de lenguaje natural y es la base de modelos como GPT, BERT, y la mayor√≠a de LLMs modernos.

### Historia Breve

**2017:** Paper "Attention is All You Need" (Google)

- Introduce la arquitectura Transformer
- Reemplaza RNNs y LSTMs

**2018-2019:** Explosi√≥n de modelos basados en Transformer

- BERT (Google)
- GPT-2 (OpenAI)

**2020-presente:** Era de LLMs

- GPT-3, GPT-4, GPT-5
- Transformers domina NLP, Vision, Multimodal

### Componentes Clave de Transformer

#### 1. Attention Mechanism (Mecanismo de Atenci√≥n)

**Concepto:**

- El modelo "presta atenci√≥n" a diferentes partes del input
- Determina qu√© palabras son relevantes para cada palabra

**Ejemplo:**

```
Frase: "El gato comi√≥ el pescado porque ten√≠a hambre"

Al procesar "ten√≠a", el modelo atiende a:
- "gato" (alta atenci√≥n) ‚Üê sujeto
- "comi√≥" (media atenci√≥n) ‚Üê verbo relacionado
- "pescado" (baja atenci√≥n) ‚Üê objeto

Conclusi√≥n: "ten√≠a" se refiere al gato, no al pescado
```

**Self-Attention:**

- Cada palabra atiende a todas las dem√°s palabras
- Captura relaciones y dependencias
- Procesamiento en paralelo (no secuencial)

#### 2. Multi-Head Attention

**Concepto:**

- M√∫ltiples "attention heads" trabajando en paralelo
- Cada head puede aprender diferentes tipos de relaciones

**Ejemplo:**

```
Head 1: Relaciones sint√°cticas (sujeto-verbo)
Head 2: Relaciones sem√°nticas (sin√≥nimos, conceptos)
Head 3: Relaciones posicionales (cerca/lejos)
...
Head 8: Otras relaciones

Resultado combinado: Comprensi√≥n rica y multidimensional
```

#### 3. Positional Encoding

**Problema:**

- Transformers procesan todo en paralelo
- No tienen noci√≥n inherente de orden/posici√≥n

**Soluci√≥n:**

- Positional encodings a√±aden informaci√≥n de posici√≥n
- Cada token recibe un vector que representa su posici√≥n

**Resultado:**

- El modelo sabe qu√© palabra viene antes/despu√©s

#### 4. Feed-Forward Networks

**Funci√≥n:**

- Procesar la informaci√≥n de attention
- Aplicar transformaciones no lineales
- A√±adir capacidad de aprendizaje

#### 5. Layer Normalization y Residual Connections

**Prop√≥sito:**

- Estabilizar entrenamiento
- Permitir redes m√°s profundas
- Prevenir vanishing gradients

### Arquitecturas Basadas en Transformer

#### 1. Encoder-Only (Solo Codificador)

**Ejemplo:** BERT

**Uso:**

- Comprensi√≥n de lenguaje
- Clasificaci√≥n
- Named Entity Recognition
- Question Answering

**Caracter√≠sticas:**

- Procesa input completo
- Bidireccional (mira adelante y atr√°s)
- Mejor para entender contexto

#### 2. Decoder-Only (Solo Decodificador)

**Ejemplo:** GPT (GPT-3, GPT-4, GPT-5)

**Uso:**

- Generaci√≥n de texto
- Chatbots
- Completaci√≥n de c√≥digo
- Traducci√≥n

**Caracter√≠sticas:**

- Genera texto token por token
- Unidireccional (solo mira hacia atr√°s)
- Autoregresivo (predice siguiente token)

#### 3. Encoder-Decoder (Codificador-Decodificador)

**Ejemplo:** T5, BART

**Uso:**

- Traducci√≥n
- Resumen
- Tareas sequence-to-sequence

**Caracter√≠sticas:**

- Encoder procesa input
- Decoder genera output
- Mejor para transformaciones complejas

### Por Qu√© Transformers Son Importantes

#### Ventajas sobre arquitecturas anteriores

**vs RNNs/LSTMs:**
‚úÖ Paralelizaci√≥n (m√°s r√°pido de entrenar)
‚úÖ Captura dependencias largas mejor
‚úÖ M√°s escalable

**vs CNNs (para NLP):**
‚úÖ Mejor para secuencias largas
‚úÖ Captura contexto global
‚úÖ M√°s flexible

#### Aplicaciones Modernas

**NLP:**

- ChatGPT, Claude, Gemini
- Traducci√≥n autom√°tica
- Resumen de documentos

**Computer Vision:**

- Vision Transformers (ViT)
- DALL-E, Stable Diffusion
- Detecci√≥n de objetos

**Multimodal:**

- GPT-4 Vision
- Flamingo
- CLIP

**Audio:**

- Whisper (transcripci√≥n)
- AudioLM (generaci√≥n)

### Conceptos Clave para el Examen

**Lo que DEBES saber:**

‚úÖ Transformer es la arquitectura base de modelos como GPT  
‚úÖ Attention mechanism permite al modelo enfocarse en partes relevantes del input  
‚úÖ Self-attention captura relaciones entre todas las palabras  
‚úÖ Multi-head attention aprende diferentes tipos de relaciones  
‚úÖ Transformers procesan en paralelo (m√°s r√°pido que RNNs)  
‚úÖ GPT usa decoder-only architecture (genera texto)  
‚úÖ Transformers se usan en NLP, Vision, Audio, Multimodal

**Lo que NO necesitas saber para AI-900:**
‚ùå Implementaci√≥n matem√°tica detallada  
‚ùå C√≥digo de implementaci√≥n  
‚ùå Detalles de backpropagation  
‚ùå Optimizaciones espec√≠ficas

---

## 4. Actualizaciones Terminol√≥gicas Importantes

### Cambios de Nombres de Servicios

| Nombre Anterior        | Nombre Actual                                   | Notas                       |
| ---------------------- | ----------------------------------------------- | --------------------------- |
| Azure AI Studio        | **Azure AI Foundry**                            | Portal principal            |
| Cognitive Services     | **Azure AI Services**                           | Nombre general de servicios |
| Azure Active Directory | **Microsoft Entra ID**                          | Servicio de identidad       |
| LUIS                   | **Conversational Language Understanding (CLU)** | Parte de Azure AI Language  |

### Nuevos T√©rminos

**Azure AI Foundry Hub:**

- Colecci√≥n de recursos compartidos
- Nivel organizacional superior a Projects

**Model Router (GPT-5):**

- Sistema que selecciona autom√°ticamente entre modelo r√°pido y de razonamiento
- Optimiza costo y rendimiento

**Reasoning Tokens:**

- Tokens usados en proceso de razonamiento interno (GPT-5)
- Se cobran aparte de prompt/completion tokens

**Provisioned Throughput Units (PTU):**

- Alternativa a pay-per-token
- Capacidad reservada y predecible

---

## 5. Temas Espec√≠ficos de la Gu√≠a Actualizada

### Identificar caracter√≠sticas de la arquitectura transformer

**Pregunta t√≠pica de examen:**
"¬øQu√© caracter√≠stica de la arquitectura transformer permite que el modelo determine qu√© partes del input son m√°s relevantes?"

**Respuesta:** Attention mechanism (Mecanismo de atenci√≥n)

### Describir las funcionalidades de Azure AI Foundry

**Temas a dominar:**

- Qu√© es Azure AI Foundry (portal unificado)
- Projects y Hubs
- Model Catalog
- Deployment options (Serverless, Managed)
- Playground
- Integration con desarrollo

### Describir las funcionalidades del cat√°logo de modelos

**Temas a dominar:**

- Qu√© modelos est√°n disponibles
- Microsoft vs Open Source vs Partner models
- C√≥mo comparar modelos
- Deployment types
- Licensing considerations

---

## ‚úÖ Puntos Clave para el Examen

### Azure AI Foundry

- [ ] Azure AI Foundry es el portal unificado para desarrollo de IA en Azure
- [ ] Incluye Projects, Hubs, Model Catalog, Playground
- [ ] Reemplaza/incluye lo que antes era Azure AI Studio
- [ ] URL: https://ai.azure.com
- [ ] Permite desplegar modelos con Serverless o Managed endpoints

### Model Catalog

- [ ] Cat√°logo curado de modelos de IA disponibles
- [ ] Incluye: Microsoft models, Open Source, Partners
- [ ] Ejemplos: GPT-5, Llama 3, Mistral, Phi-3
- [ ] Informaci√≥n por modelo: descripci√≥n, specs, licencia, pricing
- [ ] Deployment options: Serverless API, Managed Endpoints, Batch

### Arquitectura Transformer

- [ ] Base de modelos modernos como GPT
- [ ] Attention mechanism es componente clave
- [ ] Self-attention captura relaciones entre palabras
- [ ] Multi-head attention aprende diferentes relaciones
- [ ] Procesamiento en paralelo (ventaja vs RNNs)
- [ ] Decoder-only para generaci√≥n (GPT)
- [ ] Encoder-only para comprensi√≥n (BERT)

### Terminolog√≠a Actualizada

- [ ] Azure AI Foundry (antes Azure AI Studio)
- [ ] Microsoft Entra ID (antes Azure AD)
- [ ] CLU (antes LUIS)
- [ ] Model Router (GPT-5)
- [ ] PTU (Provisioned Throughput Units)

---

## üéØ Preguntas de Pr√°ctica - Temas Nuevos

### Pregunta 1

¬øQu√© portal de Azure proporciona acceso unificado a modelos de IA, incluyendo GPT, Llama, y otros modelos open source?

A) Azure Portal  
B) Azure AI Foundry  
C) Azure DevOps  
D) Azure Marketplace

**Respuesta correcta: B) Azure AI Foundry**

**Explicaci√≥n**: **Azure AI Foundry** (ai.azure.com) es el portal unificado para desarrollo de aplicaciones de IA que incluye acceso al Model Catalog con modelos de Microsoft, open source y partners. Azure Portal (A) es el portal general de Azure, DevOps (C) es para CI/CD, y Marketplace (D) es para soluciones comerciales.

---

### Pregunta 2

En el Model Catalog de Azure AI Foundry, encuentras el modelo "Llama 3 70B". ¬øQu√© tipo de modelo es este?

A) Modelo propietario de Microsoft  
B) Modelo open source de Meta  
C) Modelo de Azure OpenAI  
D) Modelo de Azure AI Vision

**Respuesta correcta: B) Modelo open source de Meta**

**Explicaci√≥n**: **Llama 3** es un modelo de lenguaje open source desarrollado por Meta (Facebook). El Model Catalog incluye modelos de m√∫ltiples proveedores: Microsoft (A es incorrecto), OpenAI (C es solo para modelos GPT/DALL-E), y otros. No es un modelo de Vision (D).

---

### Pregunta 3

¬øQu√© componente de la arquitectura transformer permite al modelo determinar qu√© palabras son m√°s relevantes al procesar una oraci√≥n?

A) Recurrent layers  
B) Convolutional filters  
C) Attention mechanism  
D) Pooling layers

**Respuesta correcta: C) Attention mechanism**

**Explicaci√≥n**: El **attention mechanism** (mecanismo de atenci√≥n) es el componente clave de transformers que permite al modelo "prestar atenci√≥n" a diferentes partes del input y determinar cu√°les son m√°s relevantes. Transformers NO usan recurrent layers (A) ni convolutional filters (B) - esas son de arquitecturas anteriores. Pooling (D) es m√°s com√∫n en CNNs.

---

### Pregunta 4

Est√°s desplegando un modelo desde el Model Catalog de Azure AI Foundry. Necesitas una soluci√≥n pay-per-use sin gestionar infraestructura. ¬øQu√© tipo de deployment debes elegir?

A) Batch Endpoints  
B) Managed Online Endpoints  
C) Serverless API  
D) On-premises deployment

**Respuesta correcta: C) Serverless API**

**Explicaci√≥n**: **Serverless API** proporciona deployment pay-per-use sin necesidad de gestionar infraestructura, con escalado autom√°tico. Batch Endpoints (A) es para procesamiento por lotes, Managed Online Endpoints (B) requiere configurar compute resources, y On-premises (D) no es una opci√≥n en el Model Catalog.

---

### Pregunta 5

¬øCu√°l es la principal ventaja de la arquitectura transformer sobre las RNNs (Recurrent Neural Networks) para procesamiento de lenguaje?

A) Transformers son m√°s peque√±os en tama√±o  
B) Transformers pueden procesar en paralelo, siendo m√°s r√°pidos de entrenar  
C) Transformers solo funcionan con ingl√©s  
D) Transformers no requieren datos de entrenamiento

**Respuesta correcta: B) Transformers pueden procesar en paralelo, siendo m√°s r√°pidos de entrenar**

**Explicaci√≥n**: La principal ventaja de **Transformers** es el **procesamiento en paralelo** gracias al attention mechanism, vs RNNs que procesan secuencialmente. Esto hace el entrenamiento mucho m√°s r√°pido y escalable. Transformers suelen ser m√°s grandes (A es falso), funcionan con m√∫ltiples idiomas (C es falso), y S√ç requieren datos (D es falso).

---

### Pregunta 6

En Azure AI Foundry, ¬øqu√© es un "Hub"?

A) Un tipo de modelo de IA  
B) Una colecci√≥n de recursos compartidos entre proyectos  
C) Un algoritmo de machine learning  
D) Un tipo de deployment endpoint

**Respuesta correcta: B) Una colecci√≥n de recursos compartidos entre proyectos**

**Explicaci√≥n**: Un **Hub** en Azure AI Foundry es un nivel organizacional que contiene recursos compartidos (connections, compute, data) que pueden ser usados por m√∫ltiples Projects. No es un modelo (A), algoritmo (C), ni endpoint (D).

---

### Pregunta 7

¬øQu√© modelo del Model Catalog de Azure AI Foundry usar√≠as para transcripci√≥n de audio a texto?

A) GPT-4  
B) DALL-E 3  
C) Whisper  
D) Llama 3

**Respuesta correcta: C) Whisper**

**Explicaci√≥n**: **Whisper** es el modelo de OpenAI espec√≠ficamente dise√±ado para speech-to-text (transcripci√≥n de audio). GPT-4 (A) es para texto/chat, DALL-E (B) para im√°genes, y Llama 3 (D) es un modelo de lenguaje general de Meta.

---

### Pregunta 8

¬øQu√© arquitectura de transformer usa GPT-5 para generaci√≥n de texto?

A) Encoder-only  
B) Decoder-only  
C) Encoder-Decoder  
D) Convolutional

**Respuesta correcta: B) Decoder-only**

**Explicaci√≥n**: GPT (Generative Pre-trained Transformer) usa arquitectura **decoder-only** que es autoregresiva - genera texto token por token mirando solo hacia atr√°s. Encoder-only (A) es usado por BERT para comprensi√≥n, Encoder-Decoder (C) por modelos como T5, y Convolutional (D) no es una arquitectura transformer.

---

### Pregunta 9

En el Model Catalog, ves que un modelo tiene licencia "MIT". ¬øQu√© significa esto?

A) El modelo es propietario y requiere pago  
B) El modelo es open source y puede usarse libremente con atribuci√≥n  
C) El modelo solo funciona en Microsoft Azure  
D) El modelo est√° restringido solo para educaci√≥n

**Respuesta correcta: B) El modelo es open source y puede usarse libremente con atribuci√≥n**

**Explicaci√≥n**: **Licencia MIT** es una licencia open source muy permisiva que permite uso comercial y modificaci√≥n con m√≠nimas restricciones (principalmente atribuci√≥n). No requiere pago (A es falso), funciona en cualquier plataforma (C es falso), y no est√° restringido a educaci√≥n (D es falso).

---

### Pregunta 10

¬øQu√© caracter√≠stica de transformers les permite capturar relaciones entre palabras que est√°n lejos unas de otras en una oraci√≥n?

A) Recurrent connections  
B) Self-attention mechanism  
C) Convolutional windows  
D) Pooling layers

**Respuesta correcta: B) Self-attention mechanism**

**Explicaci√≥n**: **Self-attention** permite que cada palabra "atienda" a todas las dem√°s palabras en la secuencia, capturando dependencias de largo alcance sin importar la distancia. RNNs con recurrent connections (A) tienen dificultad con dependencias largas, y CNNs (C, D) tienen ventanas locales limitadas.

---

## üìö Recursos Adicionales

- [Azure AI Foundry Documentation](https://learn.microsoft.com/azure/ai-foundry/)
- [Model Catalog Overview](https://learn.microsoft.com/azure/ai-foundry/how-to/model-catalog)
- [Transformer Architecture Explained](https://arxiv.org/abs/1706.03762) (Paper original)
- [Attention is All You Need - Paper](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

---

## üéì Resumen Final

### Lo NUEVO en AI-900 (Mayo 2025)

1. ‚úÖ **Azure AI Foundry** reemplaza terminolog√≠a de Azure AI Studio
2. ‚úÖ **Model Catalog** es testable - conoce los tipos de modelos
3. ‚úÖ **Arquitectura Transformer** es un nuevo tema - entiende conceptos b√°sicos
4. ‚úÖ **IA Generativa tiene el mayor peso** del examen (20-25%)

### Preparaci√≥n Final

- [ ] Revisa estos temas complementarios
- [ ] Practica preguntas sobre Azure AI Foundry
- [ ] Entiende attention mechanism a nivel conceptual
- [ ] Familiar√≠zate con t√©rminos del Model Catalog
- [ ] Conoce la diferencia entre deployment types

---

**¬°Con este contenido complementario est√°s 100% cubierto para el examen actualizado!** üöÄ
